

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Kevin Zheng">
  <meta name="keywords" content="">
  
    <meta name="description" content="本章要解决的问题：  梯度到底是什么 梯度如何被利用到神经网络中去训练神经网络的  什么是反向传播 反向传播嘛顾名思义就是把信息反方向的传播的一种方式。在这之前我们先来看什么是正向传播， 神经网络中正向传播其实就是把信息输入神经网络， 信息通过一个一个感知机计算最后输出一个结果，说是感知器实际上就是感知机中的\(W,\,b\)对结果产生的影响。而每个\(W,\,b\)对结果产生的影响的大小就要看他">
<meta property="og:type" content="article">
<meta property="og:title" content="如何理解“梯度下降法”？什么是“反向传播”？">
<meta property="og:url" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/index.html">
<meta property="og:site_name" content="Kevin的个人博客">
<meta property="og:description" content="本章要解决的问题：  梯度到底是什么 梯度如何被利用到神经网络中去训练神经网络的  什么是反向传播 反向传播嘛顾名思义就是把信息反方向的传播的一种方式。在这之前我们先来看什么是正向传播， 神经网络中正向传播其实就是把信息输入神经网络， 信息通过一个一个感知机计算最后输出一个结果，说是感知器实际上就是感知机中的\(W,\,b\)对结果产生的影响。而每个\(W,\,b\)对结果产生的影响的大小就要看他">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%206.42.40%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%206.47.32%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%206.52.42%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%206.55.14%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%207.13.22%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%207.25.01%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%207.32.18%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%207.39.39%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%207.46.02%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%207.53.22%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%204.16.58%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%204.30.36%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%204.49.28%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%204.57.50%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%205.19.17%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%205.25.42%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%205.34.03%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%205.49.08%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%205.57.32%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%206.02.36%20pm.jpg">
<meta property="og:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-17%20at%206.08.32%20pm.jpg">
<meta property="article:published_time" content="2022-02-16T05:25:07.000Z">
<meta property="article:modified_time" content="2022-02-17T10:23:08.090Z">
<meta property="article:author" content="Kevin Zheng">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="梯度">
<meta property="article:tag" content="梯度下降法">
<meta property="article:tag" content="反向传播">
<meta property="article:tag" content="偏微分">
<meta property="article:tag" content="链式反导">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2022/02/16/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/Screen%20Shot%202022-02-16%20at%206.42.40%20pm.jpg">
  
  
  <title>如何理解“梯度下降法”？什么是“反向传播”？ - Kevin的个人博客</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Kevin的个人博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="如何理解“梯度下降法”？什么是“反向传播”？">
              
            </span>

            
              <div class="mt-3">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-author" aria-hidden="true"></i>
      Kevin Zheng
    </span>
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-02-16 13:25" pubdate>
        2022年2月16日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      5.8k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      49 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">如何理解“梯度下降法”？什么是“反向传播”？</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：19 天前
                
              </p>
            
            <div class="markdown-body">
              <p>本章要解决的问题：</p>
<ol type="1">
<li>梯度到底是什么</li>
<li>梯度如何被利用到神经网络中去训练神经网络的</li>
</ol>
<h3 id="什么是反向传播">什么是反向传播</h3>
<p>反向传播嘛顾名思义就是把信息反方向的传播的一种方式。在这之前我们先来看什么是正向传播， 神经网络中正向传播其实就是把信息输入神经网络， 信息通过一个一个感知机计算最后输出一个结果，说是感知器实际上就是感知机中的<span class="math inline">\(W,\,b\)</span>对结果产生的影响。而每个<span class="math inline">\(W,\,b\)</span>对结果产生的影响的大小就要看他们具体的数值了有的对结果影响大有的对结果影响小。</p>
<p>反向传播其实和正向传播是一样的，当一个神经网络没有训练好时它的结果是又偏差的，而这个偏差也是依赖于感知机中的<span class="math inline">\(W,\,b\)</span>的数值。相对于正向传播，反向传播不只是当当反向反了而已，反的还有它的传递的信息。它传递的是偏差的信息，将这些偏差传递到一个个参数上，最后根据每个参数对偏差做出的贡献大小相应的进行修改。</p>
<h3 id="反向传播是如何修改参数的">反向传播是如何修改参数的</h3>
<p>首先我们先用一个最符合直觉方法来理解一下反向传播究竟是怎么传播的，但先打个预防针，这个方法它并不正确只是可以方便我们的理解。</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%206.42.40%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>首先这里的<span class="math inline">\(a^{[3]}\)</span>就是最后一层感知机输出的结果， <span class="math inline">\(\sigma\)</span>就是激活函数啦。<span class="math inline">\(W^{[3]},\,b^{[3]}\)</span>分别是最后一个感知机的权重和偏置，而<span class="math inline">\(a^{[2]}\)</span>是上一层隐藏层输出的结果。 这里我们使用交叉熵来计算损失值<span class="math inline">\(J\)</span>。</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%206.47.32%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>这里我们假设损失值就是个大饼，我们可以通过分配这个大饼的方式去按贡献大小分配个每个参数。在这一层中我们就可以分配个<span class="math inline">\(W^{[3]},\,b^{[3]}\)</span>然后进行修改，但是我们并不能直接修改<span class="math inline">\(a^{[2]}\)</span>所以我们就需要将这部分反向传播给上一层。</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%206.52.42%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>当然不是平均分配给每个感知机，而是需要根据每个感知机贡献的多少来进行分配，而每个感知机的参数如何调整又需要根据每个感知机的具体情况进行细分。然后我们对每个<span class="math inline">\(W^{[2]},\,b^{[2]}\)</span>再进行调整，但它需要承担的偏差也并不是由他自己决定的还需要继续向前传播。</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%206.55.14%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>而前一层的感知机所需要承担的偏差是由后面所有感知机共同决定的，这里应该是需要一个算法来决定每个感知机究竟要分配多少偏差。到目前为止我们用了一个不正常但很符合直觉的办法理解反向传播是如何分配偏差的。但这种符合直觉的方法实际上在真正数值计算上并没有那么容易，那还有别的方法吗？当然有！我们前面用这种想切西瓜的方法来进行偏差的分配其实是数值的加法，那我们是否可以考虑使用向量来解决呢？使用向量的话我们就需要考虑到它不止有数值还有反向了，这就要引出梯度这个概念。</p>
<p>其实准确来说应该是梯度的反方向。因为按照定义梯度是指向数值增加最快的方向，而反方向就是数值减少最快的方向。</p>
<h3 id="什么是梯度">什么是梯度</h3>
<p><span class="math display">\[
梯度 = \nabla f(x,\,y)
\]</span></p>
<p>这个其实就是在求<span class="math inline">\(f(x,\,y)\)</span>这个函数的梯度。我们来看下面这张图，红色的曲面其实就是<span class="math inline">\(f(x,\, y)\)</span>这个函数，可以看到我们可以选定曲线上任意一个点过这个点做切面，在这个切面上有无数个切线而最特殊的就是图中的黑线他表示在这条切线上函数的数值变化最快并且他与梯度密切相关。在左图中的这个向量就是梯度，他其实就是这条黑线在平面上的投影。梯度指向的方向就是函数在这个点上数值增加最快的方向，并且梯度永远都和等高线垂直。</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%207.13.22%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>我们现在选定一个点然后看到我们可沿着这个点做出它点梯度，因为他是个向量所以我们可以在平面中沿着坐标轴做出梯度的分量。我们假设<span class="math inline">\(x\)</span>轴和<span class="math inline">\(y\)</span>轴的单位向量分别是<span class="math inline">\(i,\,j\)</span>当我们知道了单位向量后其实我们就可以把梯度的这两个分量表达出来。<span class="math inline">\(α,\,β\)</span>分别是分量的系数，我们把分量表示出来了就可以很轻松的写出梯度的表达式了。</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%207.25.01%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>到这里我们实际上就是把梯度进行了分解，那么梯度他有个非常重要的意义就是它始终指向函数数值变化最快的方向。那么如果我们确定了一个点后我们想要沿着梯度的方向也就是数值变化最快的方向变化那也就变得非常简单了。</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%207.32.18%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>我们只需要把原来的<span class="math inline">\(x\)</span>加上一个<span class="math inline">\(\alpha\)</span>倍数，在原来的<span class="math inline">\(y\)</span>上加一个<span class="math inline">\(\β\)</span>的倍数，而这个倍数<span class="math inline">\(\eta\)</span>是相等的，所以就相当于梯度在<span class="math inline">\(x,\,y\)</span>上的分量同时放大或缩小最后的方向是不变的。</p>
<h3 id="如何在神经网络中使用梯度">如何在神经网络中使用梯度</h3>
<p>但我们已经理解了什么事梯度后我们其实就可以把原本的损失函数<span class="math inline">\(J\)</span>看成是前面的函数<span class="math inline">\(f\)</span></p>
<p><img src="./Screen%20Shot%202022-02-16%20at%207.39.39%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>那么损失函数的梯度其实就是代表它增大最快的那个方向，那如果取反就是损失值减小最快的那个方向了。</p>
<hr />
<p><strong><em>以下数学不严谨，只可意会</em></strong></p>
<hr />
<p><img src="./Screen%20Shot%202022-02-16%20at%207.46.02%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>首先这里的<span class="math inline">\(\nabla J\)</span>就是损失函数的梯度啦，<span class="math inline">\(α, \,β, \, \gamma\)</span> 分别是梯度在三个分量上的系数。那么根据前面讲的如果我们需要沿着梯度的反方向改变值的话我们就需要在原来的值基础上减去<span class="math inline">\(\eta\)</span>倍的原来的系数。那么在这一层我们就可以直接对权重和偏执进行更新，但是我们并不能直接修改<span class="math inline">\(a^{[2]}\)</span>，我们需要继续进行方向传播。那既然如此我们不如对它进行一下位置的调换</p>
<p><img src="./Screen%20Shot%202022-02-16%20at%207.53.22%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>我们就会发现<span class="math inline">\(β\)</span>其实就是目前的<span class="math inline">\(a^{[2]}\)</span>和我们期望的<span class="math inline">\(a^{[2]}\)</span>之间的差值，而我们舍去<span class="math inline">\(\eta\)</span>因为在之后我们继续进行反向传播它还会分配给之后的权重和偏置，所以我们不要在这里乘上这个倍数，因此我们先将他舍去以方便后续的计算。</p>
<p>到这里我们可以发现这里其实和损失函数有着异曲同工之妙。损失函数是表示我们期望的结果和目前的结果的差值，而这里也是表示上一层隐藏层输出的结果和我们期望的结果的差值。因此我们可以把它假设成一个<strong>“损失函数”</strong>，这里并非是真的损失函数但我们可以假设他是然后用同样的计算进行后续的操作。</p>
<p>上述的<span class="math inline">\(α, \,β, \, \gamma\)</span> 并不严谨，实际上他们都是以向量的方式表达的，具体数学在这里就不细分析了，以下给出简要的表达式（不会的回去补高数！！） <span class="math display">\[
\nabla f(x, y) \hspace{20mm}\\
=(\frac{\partial f}{\partial x},\, \frac{\partial f}{\partial y}) \hspace{10mm}\\
=\frac{\partial f}{\partial x} \cdot i + \frac{\partial y}{\partial y}\cdot j
\]</span> 这样我们就能使用偏导来代替前面的<span class="math inline">\(α, \,β, \, \gamma\)</span> 了 <span class="math display">\[
\alpha = \frac{\partial J}{\partial W^{[3]}} \\
\gamma = \frac{\partial J}{\partial b^{[3]}} \hspace{2mm}\\
\beta = \frac{\partial J}{\partial a^{[2]}} \hspace{1mm}
\]</span> 接下来我们就可以直接替换原本的<span class="math inline">\(α, \,β, \, \gamma\)</span> 用偏导代替，但在下一层中的差值就不是由当当一个感知机决定了我们就前面所有的值求平均。</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%204.16.58%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<h3 id="梯度下降法的严谨数学表达">梯度下降法的严谨数学表达</h3>
<p><img src="./Screen%20Shot%202022-02-17%20at%204.30.36%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>接下来我们就用相对严谨的数学进一步的理解梯度下降法中数据是如何反向传递的。首先我们单独拎一个感知机出来看，图中的<span class="math inline">\(a^{[l-1]}\)</span> 表示的是上一层的输出结果<span class="math inline">\(l\)</span>表示第<span class="math inline">\(l\)</span>层，这里的<span class="math inline">\(a^{[l-1]}\)</span>其实是一个矩阵，矩阵的每一个值表示的是上一层每一个感知机的输出结果。<span class="math inline">\(W_i^{[l]}\)</span>表示的是<span class="math inline">\(l\)</span>层的所有权重每个权重都是一个向量。 <span class="math inline">\(b_i^{[l]}\)</span>表示的是<span class="math inline">\(l\)</span>层的偏置。我们把上一层的所有结果乘以权重矩阵的转置加上偏置就是我们的结果<span class="math inline">\(z_i^{[l]}\)</span>，但感知机的输出结果还需要通过激活函数<span class="math inline">\(\sigma\)</span>输出结果<span class="math inline">\(a_i^{[l]}\)</span>。虽然这样的表达已经很简洁了但是，如果我们需要一一个感知机的这么写还是很复杂。</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%204.49.28%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>其实我们可以直接按整层的来开这了的<span class="math inline">\(z^{[l]}\)</span>是个矩阵代表的是<span class="math inline">\(l\)</span>层所有的输出。权重矩阵每一行表示的是当层每一个感知机的权重，每一列其实可以看成是上一层的每一个感知机的权重。<span class="math inline">\(a^{[l-1]}\)</span>表示的是上一层的所有输出结果，<span class="math inline">\(b^{[l]}\)</span>表示的是<span class="math inline">\(l\)</span>层的所有偏置。 就此我们隐藏式层算是介绍完了。接下来我们就要关注一下输出层了。</p>
<figure>
<img src="./Screen%20Shot%202022-02-17%20at%204.57.50%20pm.jpg" srcset="/img/loading.gif" lazyload alt="Screen Shot 2022-02-17 at 4.57.50 pm" /><figcaption aria-hidden="true">Screen Shot 2022-02-17 at 4.57.50 pm</figcaption>
</figure>
<p>我们知道在输出层我们需要使用损失函数<span class="math inline">\(J\)</span>来计算神经网络的输出和我们的期望的差值。这里<span class="math inline">\(a^{[l](k)}\)</span>就是神经网络输出层输出的<span class="math inline">\(k\)</span>个结果，<span class="math inline">\(y^{(k)}\)</span> 其实就是我们训练数据打的标签，对于每个<span class="math inline">\(x^[(k)]\)</span>都是人工识别标注的。在输入层中<span class="math inline">\(x\)</span> 就是我们输入的一个一个数据，<span class="math inline">\(x^{(k)}\)</span>的每一个值就是一个数据的分量，假设如果是图片数据那么<span class="math inline">\(x_1^{(k)} \cdots x_j^{(k)}\)</span>就是图片的每个像素。到此为止我们考虑的都是自由一个输出结果的二分问题，实际上我们一个扩充一下 。</p>
<h3 id="关于多输出的神经网络的反向传播">关于多输出的神经网络的反向传播</h3>
<p><img src="./Screen%20Shot%202022-02-17%20at%205.19.17%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>扩充之后输出层就不只有一个节点了而是有<span class="math inline">\(i\)</span>个节点，每个节点都有一个输出值，而这里的损失函数<span class="math inline">\(J\)</span>是对谁有的输出<span class="math inline">\(a_i^{[l](k)}\)</span>的一个统一判断的损失值。也就是整体的这个神经网络离我们的预期还差多少。 因为这里我们只考虑训练神经网络的时候，在训练神经网络的时候这里的<span class="math inline">\(y^{(k)}\)</span>和<span class="math inline">\(x^{(k)}\)</span>都是个确定的值，不是变动参量，所以这里的损失函数我们还可以再简写一下变成<span class="math inline">\(J(a_i^{[l]})\)</span>，损失函数唯一依赖的就是神经网络最后的输出值。</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%205.25.42%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>我们知道如果要进行反向传播第一步就要对损失函数求梯度，那么因为损失函数依赖于最后每个感知机输出的值，所以对于每个输出都有一个分量，而这个分量就是输出层每个感知机需要承担的偏差值。为了后面描述方便我更愿意在这里添加一个虚拟的<span class="math inline">\(l+1\)</span>层，这个<span class="math inline">\(l+1\)</span>层不是真实存在的只是为了表述方便。我们吧每一个分量当作是一个新的误差函数，而每一个误差函数只对这一个感知机有效对别的感知机无效。</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%205.34.03%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>那么我们为了继续向前传播就需要对每一个误差函数求梯度，也就是这里的<span class="math inline">\(\nabla J_1^{[l+1]} \cdots \nabla J_i^{[l+1]}\)</span>。</p>
<p>到这里我们就可以继续一步一步向前传播了，但为了理解我们继续展开来看看。首先我们需要关注的是<span class="math inline">\(\frac{\partial J_1^{[l+1]}}{\partial W_1^{[l]}}\)</span>，我们知道损失函数<span class="math inline">\(J\)</span>其实是一个关于<span class="math inline">\(a^{[l]}\)</span>的函数，而这个<span class="math inline">\(a^{[l]}\)</span>是一个关于激活函数<span class="math inline">\(\sigma\)</span>的函数，所以我们这里需要使用链式求导对她进行展开。 展开之后我们发现对每个分量都有一个<span class="math inline">\(\frac{\partial J_1^{[l+1]}}{\partial \sigma}\)</span>，对于这个我们就可以利用原本那个整体的损失函数<span class="math inline">\(J(a_i^{[l]})\)</span>带进来。就会变成： <span class="math display">\[
\nabla J_1^{[l+1]} = (\frac{ v J_1^{[l+1]}}{\partial \sigma}\frac{\partial \sigma}{\partial z_1^{[l]}}\frac{\partial z_1^{[l]}}{\partial W_1^{[l]}},\, \frac{\partial J_1^{[l+1]}}{\partial \sigma}\frac{\partial \sigma}{\partial z_1^{[l]}}\frac{\partial z_1^{[l]}}{\partial a^{[l-1]}},\, \frac{\partial J_1^{[l+1]}}{\partial \sigma}\frac{\partial \sigma}{\partial z_1^{[l]}}\frac{\partial z_1^{[l]}}{\partial b_1^{[l]}})
\]</span> 后面为了简化表达就不继续展开了。然后我们还需要关注另一就是<span class="math inline">\(\partial W_1^{[l]}\)</span>，我们知道<span class="math inline">\(W_1^{[l]}\)</span>是个向量所以它是有分量的，队友有分量的进行求偏导其实这里是有简化的。他其实相当于对每个分量进行求偏导。对于<span class="math inline">\(a^{[l-1]}\)</span>也是同理。后面也不继续展开了。</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%205.49.08%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>但我们已经知道了这些分量后我们就可以对参数进行修改了</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%205.57.32%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>对于<span class="math inline">\(W,\,b\)</span>我们就可以直接进行修改了这里的<span class="math inline">\(\eta\)</span>其实就是我们常说的学习率了，然后对于<span class="math inline">\(a\)</span>我们可以构建新的损失函数继续进行反向传播。这里我们利用了滑动窗口的思想来理解接下来的操作。</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%206.02.36%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>对于每一个损失函数求梯度，这里每一个梯度都有多个分量，每个感知机的偏差都由上一层的所有感知机共同承担。这里每一个<span class="math inline">\(a\)</span>又都是一个关于<span class="math inline">\(W,\,a,\,b\)</span>的函数，所以我们可以继续展开。</p>
<p><img src="./Screen%20Shot%202022-02-17%20at%206.08.32%20pm.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>在这里我们需要多注意的是对于<span class="math inline">\(l\)</span>层所有的感知机来说，他们需要承担的偏差值就不是由一个感知机来赋予的了，而是由所有的感知机共同赋予的。我们把所有偏差值的分量统一加起来求平均把它作为需要调整的的量。接下来我们就可以把所有的需要调整的偏差值写出来。 <span class="math display">\[
(\Delta W_1^{l},\, \Delta a^{[l-1]},\, \Delta b_1^{[l]},\,\cdots\,,\,\Delta W_i^{l},\, \Delta a^{[l-1]},\, \Delta b_i^{[l]})
\]</span></p>
<p><span class="math display">\[
(\Delta W_1^{l},\, J_1^{[l]},\, \Delta b_1^{[l]},\,\cdots\,,\,\Delta W_i^{l},\, J_i^{[l]},\, \Delta b_i^{[l]})
\]</span></p>
<p>其中<span class="math inline">\(W,\, b\)</span>就可以直接进行修改了，然后我们重新定义<span class="math inline">\(J_i^{[l]}\)</span>就可以把这个循环继续下去了。</p>
<h3 id="references">References</h3>
<p>https://www.geogebra.org/m/HpDDHprj</p>
<p>https://www.bilibili.com/video/BV1Zg411T71b?spm_id_from=333.999.0.0</p>
<hr />
<p><strong><em>知识来源作者为b站UP主王木头学科学</em></strong></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%95%B0%E5%AD%A6/">数学</a>
                    
                      <a class="hover-with-bg" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a>
                    
                      <a class="hover-with-bg" href="/tags/%E6%A2%AF%E5%BA%A6/">梯度</a>
                    
                      <a class="hover-with-bg" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/">梯度下降法</a>
                    
                      <a class="hover-with-bg" href="/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">反向传播</a>
                    
                      <a class="hover-with-bg" href="/tags/%E5%81%8F%E5%BE%AE%E5%88%86/">偏微分</a>
                    
                      <a class="hover-with-bg" href="/tags/%E9%93%BE%E5%BC%8F%E5%8F%8D%E5%AF%BC/">链式反导</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/02/19/softmax/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">用最大熵搞懂Softmax</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/02/15/%E4%BA%A4%E5%8F%89%E7%86%B5/">
                        <span class="hidden-mobile">交叉熵如何做损失函数？</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script>
        Fluid.utils.loadComments('#disqus_thread', function() {
          Fluid.utils.createCssLink('https://cdn.jsdelivr.net/npm/disqusjs@1/dist/disqusjs.css');
          Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/disqusjs@1/dist/disqus.js', function() {
            new DisqusJS({
              shortname: 'test',
              apikey: ''
            });
          });
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
