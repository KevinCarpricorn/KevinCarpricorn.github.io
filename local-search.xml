<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”</title>
    <link href="/2022/02/13/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <url>/2022/02/13/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>在吴恩达的课程中提到了两个公式分别是最小二乘法和极大似然估计 <span class="math display">\[\mathscr{L}\left(\hat{y}, y\right) = \frac{1}{2}\left(\hat{y}-y\right)^2\]</span></p><p><span class="math display">\[\mathscr{L}\left(\hat{y}, y\right) = -\left(y\log\hat{y} + \left(1-y\right)\log\left(1-\hat{y}\right)\right)\]</span></p><p>本章要解决的问题：</p><ol type="1"><li>直观的理解损失函数是什么， 为什么这么重要</li><li>吴恩达老师给出的两个损失函数分布是最小二乘法和极大似然估计，他们是怎么来的， 为什么叫这个名字</li><li>最小二乘法和极大似然估计有什么联系</li></ol><h3 id="损失函数的作用">损失函数的作用</h3><p>在弄明白损失函数是如何设计出来之前，我们得先清楚损失函数的作用是什么</p><p>首先如果我们要判断一张照片是不是猫，对于人来说是很简单的我们只需要看一眼便有了答案。如果让我们去制定一个标准来认定符合什么的是猫符合什么的不是猫其实并没有那么容易，但在我们心里我们知道是有那么一个定义或者规律的。</p><p>所以对于机器来说我们并不需要直接告诉他们去判断一个物品是什么的准确标准是什么，而是让他们学习找到这个规律。 具体过程就是先判断再比较然后调整，一直循环这个过程。而损失函数就是用来比较这个环节用来比较模型的判断和我们脑中的判断的差距有多大。</p><p>通过损失函数我们能知道当前模型和我们脑中的模型的一个差值， 然后我们可以通过比如梯度下降来把损失函数计算出来的差值分配给各个参数。使用梯度下降的好处就是我们可以知道具体哪个参数贡献的差值较多哪个较少。最后通过调整参数然后一遍一遍的判断计算损失值修改我们最后把差值降低到一定的范围之内。</p><h3 id="最小二乘法是怎么得出来的">“最小二乘法”是怎么得出来的</h3><p>想要获得真实规律和神经网络里规律的差值我们就需要设计出损失函数对他们进行比较，但我们无法将真实规律和模型的规律进行直接比较因为我们并不知道真实的规律。</p><p>庆幸的是我有已经打好标签的数据集，这些数据我们已经知道正确结果了，换句话来说这些标签其实就是真实规律的判读结果。那既然我们无法直接比较规律我们就可以比较他们判断的结果。如果我们猜测的规律就是真实规律的话， 那么神经网络的判断结果一定和数据标签一致。<span class="math inline">\(\hat{y}\)</span>是神经网络判断的结果， <span class="math inline">\(y\)</span>是标签的真实结果，那么损失函数就可以设计成， 把数据集里所有数据都放到神经网络判断一遍，挨个比较猜测的结果和真实的结果，看看它们之间差了多少，然后把所有的差值都加起来。 <span class="math display">\[\sum_{i=1}^{n} \left| \hat{y}_i-y_i \right|\]</span> 我们都知道，绝对值这个函数并不是全定义域都可导的，而随后求最小值还要进行求导，所以我们就可以把绝对值换成平方（还额外加了一个系数<span class="math inline">\(\frac{1}{2}\)</span>，这是因为求导的时候指数部分的<span class="math inline">\(2\)</span>会拿下来，可以和<span class="math inline">\(\frac{1}{2}\)</span>抵消）。 <span class="math display">\[\sum_{i=1}^{n} \frac{1}{2} \left( \hat{y}_i-y_i \right)^2\]</span> 于是寻找最接近真实规律的过程，就可以描述成是求上面这个式子最小值的过程，而这就是最小二乘法，二乘其实就是以前对平方的一种称呼。</p><h3 id="如果深度学习是个概率问题">如果深度学习是个概率问题</h3><p>第一个式子是怎么设计出来的，这个问题我解决了。那第二个式子又是如何设计出来的？它又为什么叫做极大似然估计法，似然又是个啥？</p><p>要想明白这些，就需要切换一下视角了，需要从概率的视角来看深度学习问题。</p><p>怎么切换成概率呢？我们可以先来看一下最理想的情况，比如，我们可以想象一下，把真实规律用概率分布的方式表示出来，会是什么样子。 （假如说就是判断是一个非猫即狗的二分问题）</p><p><img src="https://s2.loli.net/2022/02/15/wg5ku7TptZOxmFD.png">也就是说，只要图片是猫，那么判断的结果一定不会是狗；如果图片是狗，那么判断的结果就一定不是猫，没有任何判断错误的情况出现。（注意：“具体一个猫的图片判断结果也是猫”这个事件的概率并不等于1，“所有猫的图片都判断成猫”、“所有狗的图片都判断成狗”，它们这些事件的概率全加起来才是1）</p><p>但是我们猜测出来的规律呢？虽然仍然可以用一个概率分布来表示，但是就没有这么准了，就算是给了一张猫的图片，但是神经网络还是有概率把它判断成狗。最理想的情况，当然是让我们猜测出来的规律可以和真实规律的概率分布一模一样，但现实是，我们几乎不可能得到和真实规律一模一样的规律，只能近似。（这个原因，需要对PAC框架和VC维理论有比较深入的理解之后才能解释清楚，这里就不多解释了。可以简单的理解为，任何一种机器学习的模型能力都是有限的，所以无法学到真实规律。）</p><p>不过，不论这么样，不论是真实的规律，还是我们猜测的规律，都可以用一个条件概率分布来呈现。我们得到的数据集里面打好标签的数据，其实就是在真实规律这个概率分布下进行抽样得到的结果，而深度学习的过程，就是我们已经有了样本数据，去反推背后概率分布是什么的过程。</p><p>这就相当于，你有一个不知道正反概率是什么的硬币，抛了10次结果是7正3反，如何才能反推出这个硬币正反的真实概率。</p><h3 id="已知样本数据-如何反推概率分布">已知样本数据， 如何反推概率分布</h3><p>如何才能反推出硬币真实的概率呢？投了10次，7正3反，是不是说硬币的概率就一定是正面的概率是0.7，反面的概率是0.3呢？</p><p>这么想很符合我们的直觉，但这并不是一件板上钉钉的事情。</p><p>你可以想一下，假如说，我们的硬币是正反概率都是0.5的话，你抛10次，难道就真的能保证一定是5次正、5次反吗？不一定吧，出现6正4反，4正6反也还是挺常见的吧。更甚者，运气好到极点，10次全部是正面也是有可能的。</p><p>那么，当我们不知道硬币正反概率的时候，7正3反，就一定0.7的概率吗？也不一定，对吧。完全有可能是，硬币的概率是0.1正、0.9反，但是运气就是很好，抛出了7正3反的结果。或者是，概率本来是0.8正、0.2反，但是运气就差那么一点，抛出了7正3反。</p><p><img src="https://s2.loli.net/2022/02/15/QOl5L4mJ2xiGs3b.png"></p><p>也就是说，我们知道抛硬币的结果（抽样结果），我们没有办法唯一确定一个真实的概率（背后的规律）。就像前面看到的，7正3反的硬币结果，没有办法排除掉任何一种概率，它们都有可能。</p><p>不过，虽然我们没有办法百分百确定样本背后的概率分布原本是什么样子的，但是我们还是可以确定，最有可能情况是什么。</p><p>比如，<span class="math inline">\(C_1\)</span>~<span class="math inline">\(C_{10}\)</span>代表着10次抛硬币的结果，<span class="math inline">\(\theta\)</span>是硬币决定正反概率的属性（这个属性是未知的；也可以直接理解为硬币固有的概率属性），那么抛10次硬币有7次是正面对应的概率就是等号右边这么多。 <span class="math display">\[P\left(C_1, C_2, C_3, \cdots, C_{10}\,|\, \theta\right)=\binom{10}{7}\prod_{i=1}^{10}P\left(C_i\,|\,\theta\right)\]</span> 有了这个式子之后，我们就可以算出来，如果硬币抛出来正面朝上的概率分别是0.1、0.7和0.8的时候，要想得到抛10次硬币7次朝上的概率分布是多少。</p><p><img src="https://s2.loli.net/2022/02/15/wLtsdqhBnvKE9DN.png"></p><p>大家算一下就知道，显然当正面概率是0.7的时候，发生的概率是最大的。</p><p>所以，我们直觉上觉得抛10次硬币7次正面，硬币的概率应该是0.7，不是没有根据的，这种情况与其他的情况想比，的确是可能性最大的。其实对于任何已经知道了样本，想要反推背后的概率分布，都可以用类似的思路。这种思路，虽然没有办法百分百的知道真实的情况是什么，但是显然猜0.7是正面，这样的正确的可能性最大。对应到深度学习里面也一样，也是已知了一堆样本数据，目的是想办法反推出生成样本数据的真实概率分布。虽然没有办法百分百确定是哪一个，但是我们还是有办法确定哪一个的可能性最大。</p><p>而这个思路，就是最大似然估计法的思路，其中的“最大”这个词，对应的就是前面说的可能性最大。至于为什么是似然值，而不是概率值，这个就用解释一下似然值和概率值的区别了。</p><h3 id="似然和概率有什么不一样">“似然”和“概率”有什么不一样</h3><p>什么是似然值？首先，它也是用来表示可能性的，但是它又和概率描述的问题不一样。就比如，<span class="math inline">\(C\)</span>代表了硬币是正还是反，<span class="math inline">\(\theta\)</span>是硬币决定正反概率的属性。 <span class="math display">\[P\left(C\, | \,\theta\right)\]</span> 这是一个概率分布的前提是，<span class="math inline">\(C\)</span>是随机变量。随机变量是什么意思呢？其实就是在说，当<span class="math inline">\(\theta\)</span>是一个固定值的时候，把所有<span class="math inline">\(C\)</span>的可能取值都考虑进来，把它们对应的概率值加起来，最后的结果是归一的。 <span class="math display">\[\sum_{x\in All}P\left(C = x\,|\,\theta=a\right)=1\]</span> 但是我们可以想一下，在前面我们的问题是什么？我们面临的问题是，<span class="math inline">\(C\)</span>是一个确定的值（也就是样本已经确定了），未知的是<span class="math inline">\(\theta\)</span>。<span class="math inline">\(\theta\)</span>是一个条件，它不是随机变量，也就是说如果把全部<span class="math inline">\(\theta\)</span>的取值都考虑进来，它并不要求满足归一。也就是下式不一定等于1。 <span class="math display">\[\sum_{x\in All}P\left(C = b\,|\,\theta=x\right)\neq1\]</span> 了解这些之后，我们应该就能明白了，如果我们设计一个函数，它的变量是<span class="math inline">\(\theta\)</span>： <span class="math display">\[\mathscr{L}\left(\theta\right)=P\left(C\,|\,\theta\right)\]</span> 这个L函数的结果，虽然还是一个概率值，也能表示某个事件发生的可能性，但是它又和概率分布的概率不太一样。概率如果写成函数的话，变量一定是随机变量才对，而这里变量是条件。</p><p>而我们在已知某个抽样结果后，反推那种情况的可能性最大，其实就是在求这个L函数的最大值。</p><p>至于这个函数呢？因为和概率表达意义不同，所以就被赋予了一个新的名字，似然函数。我们说的最大似然估计法，其实就是在说，要求出似然函数的最大值，这个最大值对着的就是最有可能的规律。</p><h3 id="最大似然估计法为什么要写成这个样子">“最大似然估计法”为什么要写成这个样子</h3><p>最大似然估计法到底是什么意思，我们已经知道了，剩下的就是神经网络里面的最大似然法为什么写出来是这个样子的。 <span class="math display">\[\mathscr{L}\left(\hat{y}, y\right) = -\left(y\log\hat{y} + \left(1-y\right)\log\left(1-\hat{y}\right)\right)\]</span> 我们先来看一下前面的抛硬币的例子。在这个例子里面，我们已经知道了抛硬币的结果，求原本的硬币概率是多少。如果把抛硬币的例子和神经网络对应起来的话，抛硬币的结果对应的就是已经有的数据集<span class="math inline">\(\left&lt;x_i,\,y_i\right&gt;\)</span>，求硬币的概率<span class="math inline">\(\theta\)</span>对应到神经网络里面就是求所有的参数<span class="math inline">\(W,\,b\)</span>。</p><p>有了这个对应之后，我们就比较容易思考了，于是就有如下：</p><p><img src="https://s2.loli.net/2022/02/15/EiGltnaHBCNz5pg.jpg"></p><p>经过上面的整理之后，就得到了似然函数的表达式了。我们的目标，也就是最接近真实规律的神经网络的参数<span class="math inline">\(\left(W,\,b\right)\)</span>，其实就是求上式在取得最大值时<span class="math inline">\(W,\,b\)</span>分别等于什么。</p><p>(在求最大值的时候灰色部分是可以忽略的。特别是P(xi)，这是因为我们默认数据集是优质的数据集，数据集里的图片都是相互独立的，而且应该是等概率的。如果这部分有问题，那就需要重新整理数据集，让数据集尽可能满足这个条件。)</p><p><img src="https://s2.loli.net/2022/02/15/bOhnN2xtA37XlGD.jpg"></p><p>通过上面的推导，就可以看出来了，为什么吴恩达老师的最大似然估计法的式子要写成那个样子了。</p><h3 id="最小二乘法可以等价于最大似然估计法">“最小二乘法”可以等价于“最大似然估计法”</h3><p>本来，讲到这里，最开始我们所有的问题就都已经解决了。不论是是最小二乘法，还是极大似然估计法，它们其实都是用来比较神经网络猜测的那个规律和真实的规律的方法。</p><p>最小二乘法认为，当所有的误差的平方值最小时，神经网络里面猜测的规律与真实的规律最接近。最大似然估计法则认为，当似然值最大的时候，猜测的规律与真实的规律最接近。</p><p>如果就是这么看的话，最小二乘法好像和极大似然估计法是两套不想干的判断标准，最后选择哪个好像就是一个偏好问题。</p><p>但其实如果对最小二乘法的本质有所了解的话，就会发现从某种程度上来说，最小二乘法与最大似然估计在底层是相通的。</p><p>为什么这么说呢？</p><p>我们可以看看最小二乘法最后求出来的最值是什么。为了简化问题，我们把Y当做变量，代表不同<span class="math inline">\(W,\,b\)</span>下神经网络得出的判断结果。这样的话，损失函数就可以写成： <span class="math display">\[J\left(Y\right) = \sum_{i=1}^n\frac{1}{2}\left(Y-y_i\right)^2\]</span> 当损失函数取值最小的时候，<span class="math inline">\(Y\)</span>等于什么，我们可以通过求导的方式求出来，因为<span class="math inline">\(J\)</span>函数最小值的时候，导数一定为0。于是就有： <span class="math display">\[\frac{\mathrm{dJ} }{\mathrm{d} x} = 0 \\\Rightarrow \sum_{i=1}^n\left(Y-y_i\right) = 0\\\Rightarrow Y = \frac{\sum_{i=1}^ny_i}{n}\hspace{7mm}\\\Rightarrow Y = \bar{y}\hspace{23mm}\]</span> 从这里就可以看出来，当<span class="math inline">\(Y\)</span>等于所有<span class="math inline">\(y_i\)</span>的平均值时得到最小值。</p><p>如果只是进行到这一步的话，我们还什么都看不出来。但是我们还可以把<span class="math inline">\(Y-y_i\)</span>看做是神经网络的判断结果与真实结果的误差，也就是： <span class="math display">\[\varepsilon = Y - y_i\]</span> 那么我们是可以把数据集里每个数据对应的<span class="math inline">\(\varepsilon\)</span>看做抽样结果，也就相当于前面抛硬币例子里面7正3反的结果。这样的话，我们其实是可以利用最大似然估计法的。(注意这里用最大似人估计法时，随机变量是误差<span class="math inline">\(\varepsilon\)</span>，而前面用最大似然估计法的时候随机变量是判断结果<span class="math inline">\(y_i\)</span>，这还是有些不一样的。)</p><p>利用最大似然估计法的话，可以得到似然函数如下: <span class="math display">\[L(Y) = P\left(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n\,|\,Y\right)\\=\prod_{i=1}^{n}P(Y-y_i)\hspace{1mm}\]</span> 求最大值，其实就是下面的函数对<span class="math inline">\(Y\)</span>求导等于0： <span class="math display">\[\frac{\mathrm{dL} }{\mathrm{d} Y} = 0\]</span> 具体这里的最大值求出来是多少我们先放一下，但是在前面我们已经知道，用最小二乘法已经求出来了，当Y等于yi的平均值时，是我们的目标。而最小二乘法和极大似然估计法，它们虽然用到了不同的思路，但都是在解决同一个问题，那我们是不是可以认为，它们其实是殊途同归的，最后的答案都是<span class="math inline">\(Y\)</span>应该等于<span class="math inline">\(y_i\)</span>的平均值。</p><p>如果真的可以做出这样的假设的话，那把平均值这个答案带到最大似然估计法里面，就可以去反推一下这个概率分布是什么样子的了。带进去之后，就会发现，这个概率分布的概率密度函数如下： <span class="math display">\[f\left(\varepsilon\right)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{\varepsilon^2}{2\sigma^2}}\]</span> 这是什么？这就是正态分布啊。</p><p>于是最小二乘法和最大似然估计法的关系就变成了这样：如果我们认定神经网络得到的结果与真实情况的误差，是属于正态分布的话，那么最小二乘法与极大似然估计法是等价的。</p><p>我们都知道正态分布最开始是被高斯最先提出来的，他提出来的思路是什么？虽然细节上可能会有差别，但是大体上他就是做了类似的思考，也就是认为最小二乘法和极大似然估计法应该殊途同归，然后计算得出了正态分布的表达式。</p><p>所以，最小二乘法和极大似然估计法，虽然形式上非常不同，但是它们本质上还是相通的。只不过，最小二乘法比极大似然估计法多了一个前提，那就是它要求误差的分布属于正态分布，只有这样的时候，最小二乘法和极大似然估计法才是等价的。</p><p>其实，最大似然估计法很多人也把它称为交叉熵法，这是因为极大似然估计法和交叉熵方法是彻彻底底的等价，而不是最小二乘法这种有条件的等价。</p><h3 id="references">References</h3><p>https://www.bilibili.com/read/cv14977249?spm_id_from=333.999.0.0</p><p>https://www.bilibili.com/video/BV1Y64y1Q7hi?spm_id_from=333.999.0.0</p><p>https://www.bilibili.com/video/BV1FT4y1E74V?from=search&amp;seid=1554295885016367140&amp;spm_id_from=333.337.0.0</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学期</tag>
      
      <tag>损失函数</tag>
      
      <tag>神经网络</tag>
      
      <tag>最小二乘法</tag>
      
      <tag>极大似然估计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络模型</title>
    <link href="/2021/12/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
    <url>/2021/12/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="卷积神经网络lenet">卷积神经网络（LeNet)</h3><figure><img src="https://miro.medium.com/max/1400/1*1TI1aGBZ4dybR6__DI9dzA.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>模型结构</strong>：卷积层块， 全链接层块</p><ul><li>卷积层块：2个<strong>卷积层 + 最大池化层</strong> 的结构组成。 由于LeNet是较早的CNN， 在每个卷积层 + 池化层后多会跟一个sigmod层 来修正输出结果。 而现在用的较多的是Relu。</li><li>全连接层块：输入为二维向量。 单卷积层块的输出传入全连接层的时候会对小批量对每个样本进行扁平化（flatten）</li></ul><p>LeNet 会随着网络的加深，宽度逐渐降低，通道逐渐增多。</p><h3 id="深度卷积神经网络alexnet">深度卷积神经网络（AlexNet）</h3><h3 id="scheme-of-the-alexnet-network-used.-download-scientific-diagram"><img src="https://www.researchgate.net/publication/320052364/figure/fig1/AS:543136445198336@1506505227088/Scheme-of-the-AlexNet-network-used.png" alt="Scheme of the AlexNet network used. | Download Scientific Diagram" /></h3><p><strong>模型结构</strong>：5层卷积 + 2层全连接隐藏层 + 1层全连接输出层</p><ul><li>卷积层： 前2个用的分别是11x11和5x5的卷积核，其余的都是3x3的卷积核。 第一， 第二， 第五个卷积层后都使用了3x3，步幅为2 的最大池化层。</li><li>全连接层：2个输出个数为4096的全连接层携带着将近1GB的模型参数。</li><li>激活函数：AlexNet使用了Relu激活函数。相比于sigmod，Relu有着更简单的计算并且在不同初始化的情况下更容易训练。例如在一些特殊初始化下， sigmod在正区间的输出极度接近0， 这会导致模型很难继续更新，而Relu在正区间的值恒为1。</li><li>过拟合：AlexNet使用了丢弃法来控制模型复杂度和防止过拟合。并且它用了大量的图象增广， 包括翻转， 裁剪， 改变颜色等，来进一步防止过拟合。</li></ul><h3 id="使用重复元素的网络vgg">使用重复元素的网络（VGG）</h3><figure><img src="https://www.researchgate.net/profile/Max-Ferguson/publication/322512435/figure/fig3/AS:697390994567179@1543282378794/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png" alt="Fig. A1. The standard VGG-16 network architecture as proposed in [32].... | Download Scientific Diagram" /><figcaption aria-hidden="true">Fig. A1. The standard VGG-16 network architecture as proposed in [32].... | Download Scientific Diagram</figcaption></figure><p><strong>模型结构</strong>：VGG块 + 全连接层块</p><ul><li>VGG块：卷积层 + 池化层， 卷积层都是用相通的填充为1，3x3的卷积核接上一个步幅为2 ， 窗口为2x2的最大池化层</li><li>全连接层块：与LeNet相似</li></ul><p>VGG是个十分对称的网络，每层都成倍的增加或者减少。相比AlexNet它提供了一种简单固定的卷积模型和深度模型的构建思路。</p><h3 id="网络中的网络nin">网络中的网络（NiN）</h3><figure><img src="https://miro.medium.com/max/1400/1*fWGsLkUnDaWz7KbIlRt9Hg.png" alt="An overview of VGG16 and NiN models | by Khuyen Le | MLearning.ai | Medium" /><figcaption aria-hidden="true">An overview of VGG16 and NiN models | by Khuyen Le | MLearning.ai | Medium</figcaption></figure><p><strong>模型结构</strong> ： NiN块</p><ul><li>NiN块：AlexNet是用多个卷积层 + 全连接层输出的结构，NiN提出了另一种思路， 它通过将小块<strong>卷积层+“全连接”层</strong>串联组成网络。由于全连接层是二维而卷积层通常来说是四维的， 所以NiN块使用1x1的卷积层代替全连接层（期中空间维度（高宽）上的每一个元素相当于样本，通道相当于特征）。每个卷积层与AlexNet类似，都是11x11， 5x5， 3x3.并且每个NiN块后接一个步幅为2，窗口大小为3x3的最大池化层。</li></ul><p>相比AlexNet，NiN去掉了最后3个全连接层，使用输出通道等于标签类别的NiN块， 然后使用全局平局池化层对每个通道中所有元素求平均并直接用于分类。 这个好处是可以显著减小模型参数尺寸， 但会造成训练时间的增加。</p><h3 id="含有并行连接的网络googlenet">含有并行连接的网络（GoogLeNet）</h3><figure><img src="https://pytorch.org/assets/images/googlenet1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li>Inception块：GoogLeNet的基础块，它借鉴NiN的网络串联网络的思路。 在每个Inception块中包含4条并行线路。前3条分别使用1x1， 3x3， 5x5的卷积层来抽取不通空间尺度下的特征信息， 期中第二三条线中先使用了1x1的卷积层来减少输入通道数，以降低模型复杂度。 最后一条使用3x3的最大池化层接1x1的卷积层来改变通道数。4条线都适用合适的填充来保证输入和输出的高宽一致。</li></ul><figure><img src="https://miro.medium.com/max/2542/1*rXcdL9OV5YKlYyks9XK-wA.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h3 id="残差网络resnet">残差网络（ResNet)</h3><figure><img src="https://d2l.ai/_images/resnet-block.svg" alt="7.6. Residual Networks (ResNet) — Dive into Deep Learning 0.17.2 documentation" /><figcaption aria-hidden="true">7.6. Residual Networks (ResNet) — Dive into Deep Learning 0.17.2 documentation</figcaption></figure><ul><li>残差块：一般来说对于激活函数的输入是神经网络一层层的计算的输出结果，但是由于网络的不断加深容易出现梯度不稳定（梯度爆炸，梯度消失）。随着网络的逐渐加深，误差并不会越来越小残差块的目的就是为了解决梯度不稳定。 它通过一种跳跃的连接方式让输出结果需要参考输入结果。</li></ul><figure><img src="https://miro.medium.com/max/874/1*R-Yzqn6VLmIyITO3ZxA1sQ.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li>残差块原理：<span class="math inline">\(a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(w^{[l+2]}a^{[l+1]} + b^{[l+2]}a^{[l]})\)</span> 我们现在不考虑 <span class="math inline">\(b^{[l+2]}\)</span>, 当发生梯度消失的时候， <span class="math inline">\(w^{[l+2]}=0\)</span>, 此时<span class="math inline">\(a^{[l+2]}=g(a^{[l]})\)</span>, 相当于把第一层的输出直接经过Relu输出。并不会因为梯度消失产生负面的影响。</li></ul><h3 id="稠密连接网络densenet">稠密连接网络（DenseNet）</h3><p><img src="https://pytorch.org/assets/images/densenet1.png" alt="img" style="zoom:60%;" /></p><p><strong>模型结构</strong>：稠密层 + 过渡层</p><ul><li>稠密层：DenseNet和ResNet十分相似，区别在于DenseNet不像ResNet将前一个模块的输出直接加到模块的输出上而是直接在通道上进行叠加</li><li>过渡层：为了防止通道数一直叠加导致模型复杂度过大，过渡层通过使用1x1的卷积层减小通道数，并使用步幅为2的平均池化层减半高宽进一步降低复杂度。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle--图像分类(CIFAR-10) 基于Pytorch的实现</title>
    <link href="/2021/08/29/Kaggle%20-%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB(CIFAR-10)/"/>
    <url>/2021/08/29/Kaggle%20-%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB(CIFAR-10)/</url>
    
    <content type="html"><![CDATA[<blockquote><p>CIFAR-10 是计算机视觉领域中一个非常重要的数据集. 它由Hinton的学生Alex Krizhevsky 和 Ilya Sutskever整理的小型数据集. 其中包括 10 个不同类别的RGB 3通道 32 * 32 的图像: 飞机 (airplane)、汽车 (automobile)、鸟类 (bird)、猫 (cat)、鹿(deer)、狗(dog)、蛙类(frog)、马(horse)、船(ship) 和卡车 (truck). 我们可以直接从Kaggle官网获得数据集 https://www.kaggle.com/c/cifar-10. 其中包含一个test文件和train文件. test 文件中有60000张图像, 每个类各有6000张. train文件中包含300000万张图像, 其中只有10000张作为测试, 省下的是Kaggle为了防止人工标记数据集的额外数据.</p></blockquote><h2 id="整理数据集">1. 整理数据集</h2><p>首先我们先导入一些必要的库</p><figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> shutil<br><span class="hljs-keyword">import</span> collections<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>data_dir = <span class="hljs-string">&quot;C:\\Users\\***\\OneDrive\\桌面\\kaggle\\CIFAR-10&quot;</span><br></code></pre></td></tr></table></figure><p>我们需要对数据集进行整理以一遍训练和测试使用. 函数<strong>read_labels</strong>用来读取训练集的标签文件并以 <strong>name: label</strong> 的字典形式储存.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_labels</span>(<span class="hljs-params">file</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()[<span class="hljs-number">1</span>:]<span class="hljs-comment">#从1开始读是为了排除文件的title行</span><br>    tokens = [l.rstrip().split(<span class="hljs-string">&#x27;,&#x27;</span>) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>((name, label) <span class="hljs-keyword">for</span> name, label <span class="hljs-keyword">in</span> tokens)<br></code></pre></td></tr></table></figure><p>我们使用一种非常常规的数据处理方法将文件每个标签作为一个文件夹储存对应的图片, 但这种方法并不高效, 我们相当于把所有图片copy了一次, 当数据量非常大到时候这个方法可能会过于耗时.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">copyfile</span>(<span class="hljs-params">filename, target_dir</span>):<span class="hljs-comment">#将图片复制到对应文件夹, 如果文件夹不存在则创建文件夹</span><br>    os.makedirs(target_dir, exist_ok=<span class="hljs-literal">True</span>)<br>    shutil.copy(filename, target_dir)<br></code></pre></td></tr></table></figure><p>下面我们对所有图片进行reorganize, 其中valid_ratio是验证集样本和原始数据集样本的比 (我们需要把数据急分成两部分一部分用作验证一部分用作训练, 由于数据量并不是很小所以不需要做k折交叉验证). 让我们以 valid_ratio=0.1 为例，由于原始的训练集有 50000 张图像，因此 train_valid_test/train 路径中将有 45000 张图像⽤ 于训练，而剩下 5000 张图像将作为路径 train_valid_test/valid 中的验证集。组织数据集后，同类别的图像将被放置在同⼀⽂件夹下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reorg_train_valid</span>(<span class="hljs-params">data_dir, labels, valid_ratio</span>):<br>    n = collections.Counter(labels.values()).most_common()[-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>]<span class="hljs-comment">#每一个标签的数量</span><br>    n_valid_per_label = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, math.floor(n * valid_ratio))<span class="hljs-comment">#验证集中每个标签的数量</span><br>    label_count = &#123;&#125;<br>    <span class="hljs-keyword">for</span> train_file <span class="hljs-keyword">in</span> os.listdir(os.path.join(data_dir, <span class="hljs-string">&#x27;train&#x27;</span>)):<br>        label = labels[train_file.split(<span class="hljs-string">&#x27;.&#x27;</span>)[<span class="hljs-number">0</span>]]<br>        filename = os.path.join(data_dir, <span class="hljs-string">&#x27;train&#x27;</span>, train_file)<br>        copyfile(filename, os.path.join(data_dir, <span class="hljs-string">&#x27;train_valid_test&#x27;</span>, <span class="hljs-string">&#x27;train_valid&#x27;</span>, label))<br>        <span class="hljs-keyword">if</span> label <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> label_count <span class="hljs-keyword">or</span> label_count[label] &lt; n_valid_per_label:<br>            copyfile(filename, os.path.join(data_dir, <span class="hljs-string">&#x27;train_valid_test&#x27;</span>, <span class="hljs-string">&#x27;valid&#x27;</span>, label))<br>            label_count[label] = label_count.get(label, <span class="hljs-number">0</span>) + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            copyfile(filename, os.path.join(data_dir, <span class="hljs-string">&#x27;train_valid_test&#x27;</span>, <span class="hljs-string">&#x27;train&#x27;</span>, label))<br>    <span class="hljs-keyword">return</span> n_valid_per_label<br></code></pre></td></tr></table></figure><p>然后我们再定义一个函数对测试集进行整理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reorg_test</span>(<span class="hljs-params">data_dir</span>):<br>    <span class="hljs-keyword">for</span> test_file <span class="hljs-keyword">in</span> os.listdir(os.path.join(data_dir, <span class="hljs-string">&#x27;test&#x27;</span>)):<br>        copyfile(os.path.join(data_dir, <span class="hljs-string">&#x27;test&#x27;</span>, test_file),<br>                 os.path.join(data_dir, <span class="hljs-string">&#x27;train_valid_test&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;unknown&#x27;</span>))<br>      <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reorg_CIFAR10</span>(<span class="hljs-params">data_dir, valid_ratio</span>):<br>    labels = read_labels(os.path.join(data_dir, <span class="hljs-string">&#x27;trainLabels.csv&#x27;</span>))<br>    reorg_train_valid(data_dir, labels, valid_ratio)<br>    reorg_test(data_dir)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">128</span><br>valid_ratio = <span class="hljs-number">0.1</span><span class="hljs-comment"># 10% 的训练集数据作为验证集</span><br>reorg_CIFAR10(data_dir, valid_ratio)<br></code></pre></td></tr></table></figure><h2 id="图像增广">2. 图像增广</h2><p>为了防止过拟合我们对图像进行增强, 由于测试集只用作测试所以我们字对其做标准化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">transform_train = torchvision.transforms.Compose([<br>    torchvision.transforms.Resize(<span class="hljs-number">40</span>),<span class="hljs-comment"># 这里吧图像放大到 40*40 后在按比例取32*32是为了取局部特征</span><br>    torchvision.transforms.RandomResizedCrop(<span class="hljs-number">32</span>, scale=(<span class="hljs-number">0.64</span>, <span class="hljs-number">1.0</span>), ratio=(<span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>)),<br>    torchvision.transforms.RandomHorizontalFlip(),<span class="hljs-comment"># 垂直翻转</span><br>    torchvision.transforms.RandomVerticalFlip(),<span class="hljs-comment"># 水平翻转</span><br>    torchvision.transforms.ToTensor(),<br>    torchvision.transforms.Normalize([<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>], [<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>]),<br>    torchvision.transforms.ColorJitter(brightness=<span class="hljs-number">0.5</span>, contrast=<span class="hljs-number">0.5</span>, saturation=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>) <span class="hljs-comment"># 百分之五十的概率对曝光, 对比度, 饱和度, 色调进行变换</span><br>])<br><br>transform_test = torchvision.transforms.Compose([<br>    torchvision.transforms.ToTensor(),<br>    torchvision.transforms.Normalize([<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>], [<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>])<br>])<br></code></pre></td></tr></table></figure><h2 id="读取数据集">3. 读取数据集</h2><p>接下来问他们用torchvision.datasets.ImageFolder实例来读取不同的数据集包括每张图片的标签.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">train_ds, train_valid_ds = [<br>    torchvision.datasets.ImageFolder(<br>        os.path.join(data_dir, <span class="hljs-string">&#x27;train_valid_test&#x27;</span>, folder),<br>        transform=transform_train<br>    ) <span class="hljs-keyword">for</span> folder <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;train_valid&#x27;</span>]<br>]<br><br>valid_ds, test_ds = [<br>    torchvision.datasets.ImageFolder(<br>        os.path.join(data_dir, <span class="hljs-string">&#x27;train_valid_test&#x27;</span>, folder),<br>        transform=transform_test<br>    ) <span class="hljs-keyword">for</span> folder <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;valid&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>]<br>]<br></code></pre></td></tr></table></figure><p>使用DataLoader对数据进行图像增广的操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">train_iter, train_valid_iter = [<br>    DataLoader(<br>        dataset, batch_size, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">True</span><span class="hljs-comment"># drop last现在可有可无</span><br>    ) <span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> (train_ds, train_valid_ds)<br>]<br><br>valid_iter = DataLoader(valid_ds, batch_size, shuffle=<span class="hljs-literal">False</span>, drop_last=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">4</span>)<br>test_iter = DataLoader(test_ds, batch_size, shuffle=<span class="hljs-literal">False</span>, drop_last=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">4</span>)<br><span class="hljs-comment"># num_workers 使用多线程运行</span><br></code></pre></td></tr></table></figure><h2 id="定义模型">4.定义模型</h2><p>这里我们使用models 模块中的resnet50模型, 对于CIAFAR-10我们从头训练不使用迁移学习. 但我们需要讲最后一层全连接层的输出改成我们需要的输出类别个数. 损失函数这里使用交叉熵误差.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_net</span>():<br>    num_classes = <span class="hljs-number">10</span><br>    net = models.resnet50(pretrained=<span class="hljs-literal">False</span>)<br>    net.fc = nn.Linear(models.resnet50().fc.in_features, num_classes)<br>    <span class="hljs-keyword">return</span> net<br><br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&quot;none&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="定义训练函数">5. 定义训练函数</h2><h3 id="辅助函数">辅助函数</h3><p>对象Accumulator 用于计算所有变量之和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Accumulator</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;For accumulating sums over `n` variables.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n</span>):<br>        self.data = [<span class="hljs-number">0.0</span>] * n<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, *args</span>):<br>        self.data = [a + <span class="hljs-built_in">float</span>(b) <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.data, args)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>):<br>        self.data = [<span class="hljs-number">0.0</span>] * <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.data[idx]<br></code></pre></td></tr></table></figure><p>accuracy 用于计算预测正确的数量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">y_hat, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Compute the number of correct predictions.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(y_hat.shape) &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> y_hat.shape[<span class="hljs-number">1</span>] &gt; <span class="hljs-number">1</span>:<br>        y_hat = argmax(y_hat, axis=<span class="hljs-number">1</span>)<br>    cmp = astype(y_hat, y.dtype) == y<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(reduce_sum(astype(cmp, y.dtype)))<br></code></pre></td></tr></table></figure><p>evaluation_acc 用于计算验证准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_acc</span>(<span class="hljs-params">net, data_iter, device=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> device:<br>            device = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.parameters())).device<br><br>    metric = Accumulator(<span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(X, <span class="hljs-built_in">list</span>):<br>                X = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X]<br>            <span class="hljs-keyword">else</span>:<br>                X = X.to(device)<br>            y = y.to(device)<br>            metric.add(accuracy(net(X), y), size(y))<br>    <span class="hljs-keyword">return</span> metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>train_batch 用于对每个batch进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_batch</span>(<span class="hljs-params">net, feature, label, loss, optimizer, device</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(feature, <span class="hljs-built_in">list</span>):<br>        feature = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> feature]<br>    <span class="hljs-keyword">else</span>:<br>        feature = feature.to(device)<br>    loss = loss.to(device)<br>    net.train()<br>    net.cuda()<br>    label = label.cuda(<span class="hljs-number">0</span>)<br>    optimizer.zero_grad()<br>    pred = net(feature)<br>    l = loss(pred, label)<br>    l.<span class="hljs-built_in">sum</span>().backward()<br>    optimizer.step()<br>    train_loss_sum = l.<span class="hljs-built_in">sum</span>()<br>    train_acc_sum = (pred.argmax(dim=<span class="hljs-number">1</span>) == label).<span class="hljs-built_in">sum</span>().item()<br>    <span class="hljs-keyword">return</span> train_loss_sum, train_acc_sum<br></code></pre></td></tr></table></figure><h3 id="定义训练函数-1">定义训练函数</h3><p>接下来我们定义train函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period, lr_decay</span>):<br>    output_params = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">id</span>, net.fc.parameters()))<br>    feature_params = <span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-built_in">id</span>(p) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> output_params, net.parameters())<br>    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)<br>    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_period, lr_decay)<br>    num_batches = <span class="hljs-built_in">len</span>(train_iter)<br>    legend = [<span class="hljs-string">&#x27;train loss&#x27;</span>, <span class="hljs-string">&#x27;train acc&#x27;</span>]<br>    <span class="hljs-keyword">if</span> valid_iter <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        legend.append(<span class="hljs-string">&#x27;valid acc&#x27;</span>)<br>    net.cuda()<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        start = time.time()<br>        net.train()<br>        metric = Accumulator(<span class="hljs-number">3</span>)<br>        <span class="hljs-keyword">for</span> i, (feature, label) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_iter):<br>            l, acc = train_batch(net, feature, label, loss, optimizer, device)<br>            metric.add(l, acc, label.shape[<span class="hljs-number">0</span>])<br>        <span class="hljs-keyword">if</span> valid_iter <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            valid_acc = evaluate_acc(net, valid_iter, device)<br>        scheduler.step()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>&#x27;</span> + <span class="hljs-string">f&#x27;train loss <span class="hljs-subst">&#123;metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">2</span>]:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span> + <span class="hljs-string">f&#x27;train acc <span class="hljs-subst">&#123;metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">2</span>]:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span> + <span class="hljs-string">f&#x27; time: <span class="hljs-subst">&#123;time.time() - start&#125;</span> sec&#x27;</span>)<br>    measures = (<span class="hljs-string">f&#x27;train loss <span class="hljs-subst">&#123;metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">2</span>]:<span class="hljs-number">.3</span>f&#125;</span>, &#x27;</span><br>                <span class="hljs-string">f&#x27;train acc <span class="hljs-subst">&#123;metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">2</span>]:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>    <span class="hljs-keyword">if</span> valid_iter <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        measures += <span class="hljs-string">f&#x27;, valid acc <span class="hljs-subst">&#123;valid_acc:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span><br>    <span class="hljs-built_in">print</span>(measures + <span class="hljs-string">f&#x27; examples/sec on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="训练模型">6. 训练模型</h2><p>我们开始对模型进行训练, 这里使用GPU训练, 并且lr_period, lr_decay我们设置为4, 0.9, 意味着每4个周期学习率自乘0.9</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">device, num_epochs, lr, wd = <span class="hljs-string">&#x27;cuda&#x27;</span>, <span class="hljs-number">100</span>, <span class="hljs-number">2e-4</span>, <span class="hljs-number">5e-4</span><br>lr_period, lr_decay, net = <span class="hljs-number">4</span>, <span class="hljs-number">0.9</span>, get_net()<br>train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period, lr_decay)<br></code></pre></td></tr></table></figure><h2 id="提交结果">7. 提交结果</h2><p>但我们训练出了满意的结果后我们使用设计好的超参数和训练好的模型对测试集重新训练并且对测试集进行分类提交.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">net, preds = get_net(), []<br>train(net, train_valid_iter, <span class="hljs-literal">None</span>, num_epochs, lr, wd, device, lr_period, lr_decay)<br><span class="hljs-keyword">for</span> X, _ <span class="hljs-keyword">in</span> test_iter:<br>    y_hat = net(X.to(device))<br>    preds.extend(y_hat.argmax(dim=<span class="hljs-number">1</span>).<span class="hljs-built_in">type</span>(torch.int32).cpu().numpy())<br>sorted_ids = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(test_ds) + <span class="hljs-number">1</span>))<br>sorted_ids.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">str</span>(x))<br>df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;id&#x27;</span>: sorted_ids, <span class="hljs-string">&#x27;label&#x27;</span>: preds&#125;) <span class="hljs-comment"># 此为Kaggle要求格式</span><br>df[<span class="hljs-string">&#x27;label&#x27;</span>] = df[<span class="hljs-string">&#x27;label&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: train_valid_ds.classes[x])<br>df.to_csv(<span class="hljs-string">&#x27;C:\\Users\\***\\OneDrive\\桌面\\kaggle\\CIFAR-10\\submission.csv&#x27;</span>, index=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>最终我们会得到一个submission.csv文件我们就可以把他上传到Kaggle啦.</p><h2 id="references">References</h2><p>https://tangshusen.me/Dive-into-DL-PyTorch/#/</p><p>https://zh-v2.d2l.ai/</p><p>https://github.com/d2l-ai/d2l-zh</p><p>https://pytorch.org/docs/stable/index.html</p><p>https://blog.csdn.net/mao_hui_fei/article/details/89477938</p>]]></content>
    
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ggplot2 cheat sheet (转载)</title>
    <link href="/2021/05/11/ggplot2-cheat-sheet-%E8%BD%AC%E8%BD%BD/"/>
    <url>/2021/05/11/ggplot2-cheat-sheet-%E8%BD%AC%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<p><img src= "https://i.loli.net/2021/05/11/L8SQ9tZkW7YdRX4.png"></p><p><img src="https://i.loli.net/2021/05/11/6LzuwcQAZnqlDfg.png"></p><h2 id="reference">reference</h2><p>https://www.rstudio.com/resources/cheatsheets/</p>]]></content>
    
    
    
    <tags>
      
      <tag>ggplot2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>R语言 dnorm, pnorm, qnorm, rnorm的区别</title>
    <link href="/2021/05/09/R%E8%AF%AD%E8%A8%80-dnorm-pnorm-qnorm-rnorm%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <url>/2021/05/09/R%E8%AF%AD%E8%A8%80-dnorm-pnorm-qnorm-rnorm%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>dnorm, pnorm, qnorm, rnorm 是R语言中常用的正态分布函数. <strong>norm</strong> 指的是正态分布(也可以叫高斯分布(<strong>normal distribution</strong>)), R语言中也有其他不同的分布操作也都类似. <strong>p q d r</strong> 这里分别指的是不同的函数下面将会详细简介这不同函数在正态分布中的应用以及这是个命令在R中如何使用.</p><h2 id="dnorm">dnorm</h2><p><strong>d</strong> - 指的是概率密度函数(probability density function)</p><p>正态分布的公式: <span class="math display">\[f(x|\mu, \sigma)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\]</span> <img src="https://i.loli.net/2021/05/09/oEpTxL26XQB7AFZ.png" width="75%"></p><p>dnorm实质上是正态分布概率密度函数值. 说人话就是返回上面这个函数的值.下面我们在代码中演示下:</p><figure class="highlight r"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><pre><code class="hljs R"><span class="hljs-comment"># 输出在标准正态分布下(mean = 0, standard deviation = 1) 0 的z-sore</span><br>dnorm<span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-number">1</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.3989423</span><br><span class="hljs-comment"># 因为是标准正态分布所以mean和sd是可以省略的</span><br>dnorm<span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.3989423</span><br><span class="hljs-comment"># 如果是一个非标准正态分布如下:</span><br>dnorm<span class="hljs-punctuation">(</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">5</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-number">3</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.08065691</span><br></code></pre></td></tr></table></figure><h2 id="pnorm">pnorm</h2><p><strong>p</strong> - 指的是概率密度积分函数（从无限小到 x 的积分）(Probability density integral function)</p><p>x指的是一个z-score, 专业名词听着玄幻, 其实就是正态分布曲线下x左边的面积(概率占比), 我们知道z-score求在哪个分为数上</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs R"><span class="hljs-comment"># 标准正态分布</span><br>pnorm<span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.5 (50%)</span><br>pnorm<span class="hljs-punctuation">(</span><span class="hljs-number">2</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.9772499</span><br><span class="hljs-comment"># 非标准正态分布</span><br>pnorm<span class="hljs-punctuation">(</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">5</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-number">3</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.1586553</span><br><span class="hljs-comment"># 也可以求x右边的概率</span><br>pnorm<span class="hljs-punctuation">(</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">5</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-number">3</span><span class="hljs-punctuation">,</span> lower.tail<span class="hljs-operator">=</span><span class="hljs-literal">FALSE</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.81586553</span><br><span class="hljs-comment"># pnorm也能用来求置信区间</span><br>pnorm<span class="hljs-punctuation">(</span><span class="hljs-number">3</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> pnorm<span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.1573054</span><br></code></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/05/09/UunzrTedDcxh7Vf.png"></p><p>上图用R可以这么写</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs R">pnorm<span class="hljs-punctuation">(</span><span class="hljs-number">2</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0.9772499</span><br></code></pre></td></tr></table></figure><h2 id="qnorm">qnorm</h2><p><strong>q</strong> - 指的是分位数函数(quantile function)</p><p>简单来说它就是pnorm的反函数, 通过百分比算z-score, 我知道分位数求z-score, 例如:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs R"><span class="hljs-comment"># 在标准正态分布中求z-score</span><br>qnorm<span class="hljs-punctuation">(</span><span class="hljs-number">0.5</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 0</span><br>qnorm<span class="hljs-punctuation">(</span><span class="hljs-number">0.96</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 1.750686</span><br>qnorm<span class="hljs-punctuation">(</span><span class="hljs-number">0.99</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 2.326348</span><br></code></pre></td></tr></table></figure><h2 id="rnorm">rnorm</h2><p><strong>r</strong> - 指的是随机数函数(random function)（常用于概率仿真）</p><p>它是用来生成一组符合正态分布的随机数, 例如:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs R"><span class="hljs-comment"># 设置随机数种子</span><br>set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">1</span><span class="hljs-punctuation">)</span><br><span class="hljs-comment"># 生成5个符合标准正态分布的随机数</span><br>rnorm<span class="hljs-punctuation">(</span><span class="hljs-number">5</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078</span><br><span class="hljs-comment"># 生成10个mean=70, sd=5的正态分布随机数</span><br>rnorm<span class="hljs-punctuation">(</span><span class="hljs-number">10</span><span class="hljs-punctuation">,</span> mean<span class="hljs-operator">=</span><span class="hljs-number">70</span><span class="hljs-punctuation">,</span> sd<span class="hljs-operator">=</span><span class="hljs-number">5</span><span class="hljs-punctuation">)</span> <span class="hljs-comment"># 65.89766 72.43715 73.69162 72.87891 68.47306 77.55891 71.94922 66.89380 58.92650 75.62465</span><br></code></pre></td></tr></table></figure><p>在R语言中生成别的各种分布也都是以d, p, q, r开头, 原理和正态分布相似</p><h2 id="references">references</h2><p>http://www.360doc.com/content/18/0913/18/19913717_786412696.shtml</p><p>https://www.runoob.com/r/r-basic-operators.html</p>]]></content>
    
    
    
    <tags>
      
      <tag>R</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo d部署报错之spawn failed的解决方案</title>
    <link href="/2021/05/09/Hexo%20d%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99%E4%B9%8Bspawn%20failed%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <url>/2021/05/09/Hexo%20d%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99%E4%B9%8Bspawn%20failed%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<p>关于Hexo部署的时候报错导致无法推送到github估计是很多小伙伴第一次接触Hexo框架编写博客的常见问题, 下面介绍两种解决方案.</p><p><img src="https://i.loli.net/2021/05/09/fsRDw1AS2VpO35o.png"></p><h2 id="解决方案一">解决方案(一)</h2><ol type="1"><li>在博客文件夹(通常是***)中删除时 <strong>.deploy_git</strong> 文件</li><li>命令行(terminal)[不推荐使用<strong>cmd</strong>, 使用 <strong>git bash</strong> 等] 中输入 <code>git config --global core.autocrlf false</code>把git加入系统环境变量</li><li>重新执行<code>hexo c</code> <code>hexo g</code> <code>hexo d</code></li></ol><p>上Google百度一查大部分都是这种方法, xdm可以自己试试看万一成了呢. 但我下面推荐另一种可能的解决方案</p><h2 id="解决方案二">解决方案(二)</h2><ol type="1"><li><p>首先用文本编辑器(我使用的是Notepad++)打开博客文件夹(通常是***)中的 **_config.yml** 配置文件</p></li><li><p>修改配置文件中的<strong>repo</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><pre><code class="hljs dts"><span class="hljs-meta"># Deployment</span><br><span class="hljs-meta">## Docs: https:<span class="hljs-comment">//hexo.io/docs/one-command-deployment</span></span><br><span class="hljs-symbol">deploy:</span><br><span class="hljs-symbol">  type:</span> git<br><span class="hljs-symbol">  repo:</span>https:<span class="hljs-comment">//github.com/YourName/YourName.github.io.git(不要使用这个)</span><br>  git@github.com:YourName/YourName.github.io.git(用这个)<br><span class="hljs-symbol">  branch:</span> master<br></code></pre></td></tr></table></figure></li><li><p>重新执行<code>hexo c</code> <code>hexo g</code> <code>hexo d</code></p></li></ol><p>这样就大功告成啦, 很简单吧, 继续写你的博客吧!</p><h2 id="reference">reference</h2><p>https://blog.zhheo.com/p/128998ac.html</p><p>https://blog.csdn.net/njc_sec/article/details/89021083</p>]]></content>
    
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
