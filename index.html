<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-卷积神经网络模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="dt-published" datetime="2021-12-03T19:08:51.000Z" itemprop="datePublished">2021-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">卷积神经网络模型</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="卷积神经网络（LeNet"><a href="#卷积神经网络（LeNet" class="headerlink" title="卷积神经网络（LeNet)"></a>卷积神经网络（LeNet)</h3><p>![Screen Shot 2021-12-05 at 12.25.06 pm](&#x2F;Users&#x2F;kevin&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;Screen Shot 2021-12-05 at 12.25.06 pm.jpg)</p>
<p><strong>模型结构</strong>：卷积层块， 全链接层块</p>
<ul>
<li>卷积层块：2个<strong>卷积层 + 最大池化层</strong> 的结构组成。 由于LeNet是较早的CNN， 在每个卷积层 + 池化层后多会跟一个sigmod层 来修正输出结果。 而现在用的较多的是Relu。</li>
<li>全连接层块：输入为二维向量。 单卷积层块的输出传入全连接层的时候会对小批量对每个样本进行扁平化（flatten）</li>
</ul>
<p>LeNet 会随着网络的加深，宽度逐渐降低，通道逐渐增多。</p>
<h3 id="深度卷积神经网络（AlexNet）"><a href="#深度卷积神经网络（AlexNet）" class="headerlink" title="深度卷积神经网络（AlexNet）"></a>深度卷积神经网络（AlexNet）</h3><h3 id="Screen-Shot-2021-12-05-at-12-24-03-pm-x2F-Users-x2F-kevin-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-Screen-Shot-2021-12-05-at-12-24-03-pm-jpg"><a href="#Screen-Shot-2021-12-05-at-12-24-03-pm-x2F-Users-x2F-kevin-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-Screen-Shot-2021-12-05-at-12-24-03-pm-jpg" class="headerlink" title="![Screen Shot 2021-12-05 at 12.24.03 pm](&#x2F;Users&#x2F;kevin&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;Screen Shot 2021-12-05 at 12.24.03 pm.jpg)"></a>![Screen Shot 2021-12-05 at 12.24.03 pm](&#x2F;Users&#x2F;kevin&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;Screen Shot 2021-12-05 at 12.24.03 pm.jpg)</h3><p><strong>模型结构</strong>：5层卷积 + 2层全连接隐藏层 + 1层全连接输出层</p>
<ul>
<li>卷积层： 前2个用的分别是11x11和5x5的卷积核，其余的都是3x3的卷积核。 第一， 第二， 第五个卷积层后都使用了3x3，步幅为2 的最大池化层。</li>
<li>全连接层：2个输出个数为4096的全连接层携带着将近1GB的模型参数。 </li>
<li>激活函数：AlexNet使用了Relu激活函数。相比于sigmod，Relu有着更简单的计算并且在不同初始化的情况下更容易训练。例如在一些特殊初始化下， sigmod在正区间的输出极度接近0， 这会导致模型很难继续更新，而Relu在正区间的值恒为1。</li>
<li>过拟合：AlexNet使用了丢弃法来控制模型复杂度和防止过拟合。并且它用了大量的图象增广， 包括翻转， 裁剪， 改变颜色等，来进一步防止过拟合。</li>
</ul>
<h3 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h3><img src="/Users/kevin/Library/Application Support/typora-user-images/Screen Shot 2021-12-05 at 12.29.11 pm.jpg" alt="Screen Shot 2021-12-05 at 12.29.11 pm" style="zoom:50%;" />

<p><strong>模型结构</strong>：VGG块 + 全连接层块</p>
<ul>
<li>VGG块：卷积层 + 池化层， 卷积层都是用相通的填充为1，3x3的卷积核接上一个步幅为2 ， 窗口为2x2的最大池化层</li>
<li>全连接层块：与LeNet相似</li>
</ul>
<p>VGG是个十分对称的网络，每层都成倍的增加或者减少。相比AlexNet它提供了一种简单固定的卷积模型和深度模型的构建思路。</p>
<h3 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h3><p>![Screen Shot 2021-12-05 at 12.44.22 pm](&#x2F;Users&#x2F;kevin&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;Screen Shot 2021-12-05 at 12.44.22 pm.jpg)</p>
<p><strong>模型结构</strong> ： NiN块</p>
<ul>
<li>NiN块：AlexNet是用多个卷积层 + 全连接层输出的结构，NiN提出了另一种思路， 它通过将小块<strong>卷积层+“全连接”层</strong>串联组成网络。由于全连接层是二维而卷积层通常来说是四维的， 所以NiN块使用1x1的卷积层代替全连接层（期中空间维度（高宽）上的每一个元素相当于样本，通道相当于特征）。每个卷积层与AlexNet类似，都是11x11， 5x5， 3x3.并且每个NiN块后接一个步幅为2，窗口大小为3x3的最大池化层。</li>
</ul>
<p>相比AlexNet，NiN去掉了最后3个全连接层，使用输出通道等于标签类别的NiN块， 然后使用全局平局池化层对每个通道中所有元素求平均并直接用于分类。 这个好处是可以显著减小模型参数尺寸， 但会造成训练时间的增加。 </p>
<h3 id="含有并行连接的网络（GoogLeNet）"><a href="#含有并行连接的网络（GoogLeNet）" class="headerlink" title="含有并行连接的网络（GoogLeNet）"></a>含有并行连接的网络（GoogLeNet）</h3><p><img src="https://pytorch.org/assets/images/googlenet1.png" alt="img"></p>
<ul>
<li>Inception块：GoogLeNet的基础块，它借鉴NiN的网络串联网络的思路。 在每个Inception块中包含4条并行线路。前3条分别使用1x1， 3x3， 5x5的卷积层来抽取不通空间尺度下的特征信息， 期中第二三条线中先使用了1x1的卷积层来减少输入通道数，以降低模型复杂度。 最后一条使用3x3的最大池化层接1x1的卷积层来改变通道数。4条线都适用合适的填充来保证输入和输出的高宽一致。</li>
</ul>
<p><img src="https://miro.medium.com/max/2542/1*rXcdL9OV5YKlYyks9XK-wA.png" alt="img"></p>
<h3 id="残差网络（ResNet"><a href="#残差网络（ResNet" class="headerlink" title="残差网络（ResNet)"></a>残差网络（ResNet)</h3><p>![Screen Shot 2021-12-05 at 2.37.01 pm](&#x2F;Users&#x2F;kevin&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;Screen Shot 2021-12-05 at 2.37.01 pm.jpg)</p>
<ul>
<li>残差块：一般来说对于激活函数的输入是神经网络一层层的计算的输出结果，但是由于网络的不断加深容易出现梯度不稳定（梯度爆炸，梯度消失）。随着网络的逐渐加深，误差并不会越来越小残差块的目的就是为了解决梯度不稳定。 它通过一种跳跃的连接方式让输出结果需要参考输入结果。</li>
</ul>
<p><img src="https://miro.medium.com/max/874/1*R-Yzqn6VLmIyITO3ZxA1sQ.png" alt="img"></p>
<ul>
<li>残差块原理：$a^{[l+2]}&#x3D;g(z^{[l+2]}+a^{[l]})&#x3D;g(w^{[l+2]}a^{[l+1]} + b^{[l+2]}a^{[l]})$ 我们现在不考虑 $b^{[l+2]}$, 当发生梯度消失的时候， $w^{[l+2]}&#x3D;0$, 此时$a^{[l+2]}&#x3D;g(a^{[l]})$, 相当于把第一层的输出直接经过Relu输出。并不会因为梯度消失产生负面的影响。</li>
</ul>
<h3 id="稠密连接网络（DenseNet）"><a href="#稠密连接网络（DenseNet）" class="headerlink" title="稠密连接网络（DenseNet）"></a>稠密连接网络（DenseNet）</h3><img src="https://pytorch.org/assets/images/densenet1.png" alt="img" style="zoom:60%;" />

<p><strong>模型结构</strong>：稠密层 + 过渡层</p>
<ul>
<li>稠密层：DenseNet和ResNet十分相似，区别在于DenseNet不像ResNet将前一个模块的输出直接加到模块的输出上而是直接在通道上进行叠加</li>
<li>过渡层：为了防止通道数一直叠加导致模型复杂度过大，过渡层通过使用1x1的卷积层减小通道数，并使用步幅为2的平均池化层减半高宽进一步降低复杂度。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/12/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/" data-id="ckzi5l9pa0005i5scfc81gywy" data-title="卷积神经网络模型" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Kaggle - 图像分类(CIFAR-10)" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/08/29/Kaggle%20-%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB(CIFAR-10)/" class="article-date">
  <time class="dt-published" datetime="2021-08-29T09:26:15.000Z" itemprop="datePublished">2021-08-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/08/29/Kaggle%20-%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB(CIFAR-10)/">Kaggle--图像分类(CIFAR-10) 基于Pytorch的实现</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p>CIFAR-10 是计算机视觉领域中一个非常重要的数据集. 它由Hinton的学生Alex Krizhevsky 和 Ilya Sutskever整理的小型数据集. 其中包括 10 个不同类别的RGB 3通道 32 * 32 的图像: 飞机 (airplane)、汽车 (automobile)、鸟类 (bird)、猫 (cat)、鹿(deer)、狗(dog)、蛙类(frog)、马(horse)、船(ship) 和卡车 (truck). 我们可以直接从Kaggle官网获得数据集 <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/cifar-10">https://www.kaggle.com/c/cifar-10</a>. 其中包含一个test文件和train文件.  test 文件中有60000张图像, 每个类各有6000张. train文件中包含300000万张图像, 其中只有10000张作为测试, 省下的是Kaggle为了防止人工标记数据集的额外数据. </p>
</blockquote>
<h2 id="1-整理数据集"><a href="#1-整理数据集" class="headerlink" title="1. 整理数据集"></a>1. 整理数据集</h2><p>首先我们先导入一些必要的库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">&quot;C:\\Users\\***\\OneDrive\\桌面\\kaggle\\CIFAR-10&quot;</span></span><br></pre></td></tr></table></figure>

<p>我们需要对数据集进行整理以一遍训练和测试使用. 函数<strong>read_labels</strong>用来读取训练集的标签文件并以 <strong>name: label</strong> 的字典形式储存.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_labels</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()[<span class="number">1</span>:]	<span class="comment">#从1开始读是为了排除文件的title行</span></span><br><span class="line">    tokens = [l.rstrip().split(<span class="string">&#x27;,&#x27;</span>) <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>((name, label) <span class="keyword">for</span> name, label <span class="keyword">in</span> tokens)</span><br></pre></td></tr></table></figure>

<p>我们使用一种非常常规的数据处理方法将文件每个标签作为一个文件夹储存对应的图片, 但这种方法并不高效, 我们相当于把所有图片copy了一次, 当数据量非常大到时候这个方法可能会过于耗时.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">copyfile</span>(<span class="params">filename, target_dir</span>):	<span class="comment">#将图片复制到对应文件夹, 如果文件夹不存在则创建文件夹</span></span><br><span class="line">    os.makedirs(target_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    shutil.copy(filename, target_dir)</span><br></pre></td></tr></table></figure>

<p>下面我们对所有图片进行reorganize, 其中valid_ratio是验证集样本和原始数据集样本的比 (我们需要把数据急分成两部分一部分用作验证一部分用作训练, 由于数据量并不是很小所以不需要做k折交叉验证). 让我们以 valid_ratio&#x3D;0.1 为例，由于原始的训练集有 50000 张图像，因此 train_valid_test&#x2F;train 路径中将有 45000 张图像⽤ 于训练，而剩下 5000 张图像将作为路径 train_valid_test&#x2F;valid 中的验证集。组织数据集后，同类别的图像将被放置在同⼀⽂件夹下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_train_valid</span>(<span class="params">data_dir, labels, valid_ratio</span>):</span><br><span class="line">    n = collections.Counter(labels.values()).most_common()[-<span class="number">1</span>][<span class="number">1</span>]	<span class="comment">#每一个标签的数量</span></span><br><span class="line">    n_valid_per_label = <span class="built_in">max</span>(<span class="number">1</span>, math.floor(n * valid_ratio))	<span class="comment">#验证集中每个标签的数量</span></span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> train_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>)):</span><br><span class="line">        label = labels[train_file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]]</span><br><span class="line">        filename = os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>, train_file)</span><br><span class="line">        copyfile(filename, os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;train_valid&#x27;</span>, label))</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count <span class="keyword">or</span> label_count[label] &lt; n_valid_per_label:</span><br><span class="line">            copyfile(filename, os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>, label))</span><br><span class="line">            label_count[label] = label_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            copyfile(filename, os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, label))</span><br><span class="line">    <span class="keyword">return</span> n_valid_per_label</span><br></pre></td></tr></table></figure>

<p>然后我们再定义一个函数对测试集进行整理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_test</span>(<span class="params">data_dir</span>):</span><br><span class="line">    <span class="keyword">for</span> test_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>)):</span><br><span class="line">        copyfile(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>, test_file),</span><br><span class="line">                 os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;unknown&#x27;</span>))</span><br><span class="line">      </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_CIFAR10</span>(<span class="params">data_dir, valid_ratio</span>):</span><br><span class="line">    labels = read_labels(os.path.join(data_dir, <span class="string">&#x27;trainLabels.csv&#x27;</span>))</span><br><span class="line">    reorg_train_valid(data_dir, labels, valid_ratio)</span><br><span class="line">    reorg_test(data_dir)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">valid_ratio = <span class="number">0.1</span>	<span class="comment"># 10% 的训练集数据作为验证集</span></span><br><span class="line">reorg_CIFAR10(data_dir, valid_ratio)</span><br></pre></td></tr></table></figure>

<h2 id="2-图像增广"><a href="#2-图像增广" class="headerlink" title="2. 图像增广"></a>2. 图像增广</h2><p>为了防止过拟合我们对图像进行增强, 由于测试集只用作测试所以我们字对其做标准化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">transform_train = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize(<span class="number">40</span>),<span class="comment"># 这里吧图像放大到 40*40 后在按比例取32*32是为了取局部特征</span></span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">32</span>, scale=(<span class="number">0.64</span>, <span class="number">1.0</span>), ratio=(<span class="number">1.0</span>, <span class="number">1.0</span>)),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),	<span class="comment"># 垂直翻转</span></span><br><span class="line">    torchvision.transforms.RandomVerticalFlip(),	<span class="comment"># 水平翻转</span></span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>], [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>]),</span><br><span class="line">    torchvision.transforms.ColorJitter(brightness=<span class="number">0.5</span>, contrast=<span class="number">0.5</span>, saturation=<span class="number">0.5</span>, hue=<span class="number">0.5</span>) <span class="comment"># 百分之五十的概率对曝光, 对比度, 饱和度, 色调进行变换</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">transform_test = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>], [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h2 id="3-读取数据集"><a href="#3-读取数据集" class="headerlink" title="3. 读取数据集"></a>3. 读取数据集</h2><p>接下来问他们用torchvision.datasets.ImageFolder实例来读取不同的数据集包括每张图片的标签.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">train_ds, train_valid_ds = [</span><br><span class="line">    torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">        transform=transform_train</span><br><span class="line">    ) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;train_valid&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">valid_ds, test_ds = [</span><br><span class="line">    torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">        transform=transform_test</span><br><span class="line">    ) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;valid&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>使用DataLoader对数据进行图像增广的操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_iter, train_valid_iter = [</span><br><span class="line">    DataLoader(</span><br><span class="line">        dataset, batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>	<span class="comment"># drop last现在可有可无</span></span><br><span class="line">    ) <span class="keyword">for</span> dataset <span class="keyword">in</span> (train_ds, train_valid_ds)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">valid_iter = DataLoader(valid_ds, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">test_iter = DataLoader(test_ds, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># num_workers 使用多线程运行</span></span><br></pre></td></tr></table></figure>

<h2 id="4-定义模型"><a href="#4-定义模型" class="headerlink" title="4.定义模型"></a>4.定义模型</h2><p>这里我们使用models 模块中的resnet50模型, 对于CIAFAR-10我们从头训练不使用迁移学习. 但我们需要讲最后一层全连接层的输出改成我们需要的输出类别个数. 损失函数这里使用交叉熵误差.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    num_classes = <span class="number">10</span></span><br><span class="line">    net = models.resnet50(pretrained=<span class="literal">False</span>)</span><br><span class="line">    net.fc = nn.Linear(models.resnet50().fc.in_features, num_classes)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="5-定义训练函数"><a href="#5-定义训练函数" class="headerlink" title="5. 定义训练函数"></a>5. 定义训练函数</h2><h3 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h3><p>对象Accumulator 用于计算所有变量之和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;For accumulating sums over `n` variables.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br></pre></td></tr></table></figure>

<p>accuracy 用于计算预测正确的数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the number of correct predictions.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = argmax(y_hat, axis=<span class="number">1</span>)</span><br><span class="line">    cmp = astype(y_hat, y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(reduce_sum(astype(cmp, y.dtype)))</span><br></pre></td></tr></table></figure>

<p>evaluation_acc 用于计算验证准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_acc</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line"></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(accuracy(net(X), y), size(y))</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>train_batch 用于对每个batch进行训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_batch</span>(<span class="params">net, feature, label, loss, optimizer, device</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(feature, <span class="built_in">list</span>):</span><br><span class="line">        feature = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> feature]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        feature = feature.to(device)</span><br><span class="line">    loss = loss.to(device)</span><br><span class="line">    net.train()</span><br><span class="line">    net.cuda()</span><br><span class="line">    label = label.cuda(<span class="number">0</span>)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    pred = net(feature)</span><br><span class="line">    l = loss(pred, label)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    train_loss_sum = l.<span class="built_in">sum</span>()</span><br><span class="line">    train_acc_sum = (pred.argmax(dim=<span class="number">1</span>) == label).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="keyword">return</span> train_loss_sum, train_acc_sum</span><br></pre></td></tr></table></figure>

<h3 id="定义训练函数"><a href="#定义训练函数" class="headerlink" title="定义训练函数"></a>定义训练函数</h3><p>接下来我们定义train函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period, lr_decay</span>):</span><br><span class="line">    output_params = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, net.fc.parameters()))</span><br><span class="line">    feature_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> output_params, net.parameters())</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_period, lr_decay)</span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_iter)</span><br><span class="line">    legend = [<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        legend.append(<span class="string">&#x27;valid acc&#x27;</span>)</span><br><span class="line">    net.cuda()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        net.train()</span><br><span class="line">        metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (feature, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            l, acc = train_batch(net, feature, label, loss, optimizer, device)</span><br><span class="line">            metric.add(l, acc, label.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            valid_acc = evaluate_acc(net, valid_iter, device)</span><br><span class="line">        scheduler.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>&#x27;</span> + <span class="string">f&#x27;train loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span> + <span class="string">f&#x27;train acc <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span> + <span class="string">f&#x27; time: <span class="subst">&#123;time.time() - start&#125;</span> sec&#x27;</span>)</span><br><span class="line">    measures = (<span class="string">f&#x27;train loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">                <span class="string">f&#x27;train acc <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        measures += <span class="string">f&#x27;, valid acc <span class="subst">&#123;valid_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(measures + <span class="string">f&#x27; examples/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="6-训练模型"><a href="#6-训练模型" class="headerlink" title="6. 训练模型"></a>6. 训练模型</h2><p>我们开始对模型进行训练, 这里使用GPU训练, 并且lr_period, lr_decay我们设置为4, 0.9, 意味着每4个周期学习率自乘0.9</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device, num_epochs, lr, wd = <span class="string">&#x27;cuda&#x27;</span>, <span class="number">100</span>, <span class="number">2e-4</span>, <span class="number">5e-4</span></span><br><span class="line">lr_period, lr_decay, net = <span class="number">4</span>, <span class="number">0.9</span>, get_net()</span><br><span class="line">train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period, lr_decay)</span><br></pre></td></tr></table></figure>

<h2 id="7-提交结果"><a href="#7-提交结果" class="headerlink" title="7. 提交结果"></a>7. 提交结果</h2><p>但我们训练出了满意的结果后我们使用设计好的超参数和训练好的模型对测试集重新训练并且对测试集进行分类提交.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net, preds = get_net(), []</span><br><span class="line">train(net, train_valid_iter, <span class="literal">None</span>, num_epochs, lr, wd, device, lr_period, lr_decay)</span><br><span class="line"><span class="keyword">for</span> X, _ <span class="keyword">in</span> test_iter:</span><br><span class="line">    y_hat = net(X.to(device))</span><br><span class="line">    preds.extend(y_hat.argmax(dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.int32).cpu().numpy())</span><br><span class="line">sorted_ids = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_ds) + <span class="number">1</span>))</span><br><span class="line">sorted_ids.sort(key=<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x))</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;id&#x27;</span>: sorted_ids, <span class="string">&#x27;label&#x27;</span>: preds&#125;) <span class="comment"># 此为Kaggle要求格式</span></span><br><span class="line">df[<span class="string">&#x27;label&#x27;</span>] = df[<span class="string">&#x27;label&#x27;</span>].apply(<span class="keyword">lambda</span> x: train_valid_ds.classes[x])</span><br><span class="line">df.to_csv(<span class="string">&#x27;C:\\Users\\***\\OneDrive\\桌面\\kaggle\\CIFAR-10\\submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>最终我们会得到一个submission.csv文件我们就可以把他上传到Kaggle啦.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></p>
<p><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/">https://zh-v2.d2l.ai/</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/mao_hui_fei/article/details/89477938">https://blog.csdn.net/mao_hui_fei/article/details/89477938</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/08/29/Kaggle%20-%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB(CIFAR-10)/" data-id="ckzi5l9p90003i5sc9cw66db7" data-title="Kaggle--图像分类(CIFAR-10) 基于Pytorch的实现" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ggplot2-cheat-sheet-转载" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/11/ggplot2-cheat-sheet-%E8%BD%AC%E8%BD%BD/" class="article-date">
  <time class="dt-published" datetime="2021-05-10T17:13:57.000Z" itemprop="datePublished">2021-05-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/11/ggplot2-cheat-sheet-%E8%BD%AC%E8%BD%BD/">ggplot2 cheat sheet (转载)</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <img src= "https://i.loli.net/2021/05/11/L8SQ9tZkW7YdRX4.png">

<img src="https://i.loli.net/2021/05/11/6LzuwcQAZnqlDfg.png">



<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p><a target="_blank" rel="noopener" href="https://www.rstudio.com/resources/cheatsheets/">https://www.rstudio.com/resources/cheatsheets/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/11/ggplot2-cheat-sheet-%E8%BD%AC%E8%BD%BD/" data-id="ckzi5l9pa0004i5scab504z72" data-title="ggplot2 cheat sheet (转载)" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ggplot2/" rel="tag">ggplot2</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-R语言-dnorm-pnorm-qnorm-rnorm的区别" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/09/R%E8%AF%AD%E8%A8%80-dnorm-pnorm-qnorm-rnorm%E7%9A%84%E5%8C%BA%E5%88%AB/" class="article-date">
  <time class="dt-published" datetime="2021-05-09T12:02:30.000Z" itemprop="datePublished">2021-05-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/09/R%E8%AF%AD%E8%A8%80-dnorm-pnorm-qnorm-rnorm%E7%9A%84%E5%8C%BA%E5%88%AB/">R语言 dnorm, pnorm, qnorm, rnorm的区别</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>dnorm, pnorm, qnorm, rnorm 是R语言中常用的正态分布函数. <strong>norm</strong> 指的是正态分布(也可以叫高斯分布(<strong>normal distribution</strong>)), R语言中也有其他不同的分布操作也都类似. <strong>p q d r</strong> 这里分别指的是不同的函数下面将会详细简介这不同函数在正态分布中的应用以及这是个命令在R中如何使用.</p>
<h2 id="dnorm"><a href="#dnorm" class="headerlink" title="dnorm"></a>dnorm</h2><p><strong>d</strong> - 指的是概率密度函数(probability density function) </p>
<p>正态分布的公式:<br>$$<br>f(x|\mu, \sigma)&#x3D;\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}<br>$$<br><img src="https://i.loli.net/2021/05/09/oEpTxL26XQB7AFZ.png" width="75%"></p>
<p>dnorm实质上是正态分布概率密度函数值. 说人话就是返回上面这个函数的值.下面我们在代码中演示下:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出在标准正态分布下(mean = 0, standard deviation = 1) 0 的z-sore</span></span><br><span class="line">dnorm<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> mean<span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> sd<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 0.3989423</span></span><br><span class="line"><span class="comment"># 因为是标准正态分布所以mean和sd是可以省略的</span></span><br><span class="line">dnorm<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">)</span> <span class="comment"># 0.3989423</span></span><br><span class="line"><span class="comment"># 如果是一个非标准正态分布如下:</span></span><br><span class="line">dnorm<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">,</span> mean<span class="operator">=</span><span class="number">5</span><span class="punctuation">,</span> sd<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span> <span class="comment"># 0.08065691</span></span><br></pre></td></tr></table></figure>

<h2 id="pnorm"><a href="#pnorm" class="headerlink" title="pnorm"></a>pnorm</h2><p><strong>p</strong> - 指的是概率密度积分函数（从无限小到 x 的积分）(Probability density integral function)</p>
<p>x指的是一个z-score, 专业名词听着玄幻, 其实就是正态分布曲线下x左边的面积(概率占比), 我们知道z-score求在哪个分为数上</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准正态分布</span></span><br><span class="line">pnorm<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">)</span> <span class="comment"># 0.5 (50%)</span></span><br><span class="line">pnorm<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 0.9772499</span></span><br><span class="line"><span class="comment"># 非标准正态分布</span></span><br><span class="line">pnorm<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">,</span> mean<span class="operator">=</span><span class="number">5</span><span class="punctuation">,</span> sd<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span> <span class="comment"># 0.1586553</span></span><br><span class="line"><span class="comment"># 也可以求x右边的概率</span></span><br><span class="line">pnorm<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">,</span> mean<span class="operator">=</span><span class="number">5</span><span class="punctuation">,</span> sd<span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> lower.tail<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span> <span class="comment"># 0.81586553</span></span><br><span class="line"><span class="comment"># pnorm也能用来求置信区间</span></span><br><span class="line">pnorm<span class="punctuation">(</span><span class="number">3</span><span class="punctuation">)</span> <span class="operator">-</span> pnorm<span class="punctuation">(</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 0.1573054</span></span><br></pre></td></tr></table></figure>

<img src="https://i.loli.net/2021/05/09/UunzrTedDcxh7Vf.png">

<p>上图用R可以这么写</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pnorm<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 0.9772499</span></span><br></pre></td></tr></table></figure>

<h2 id="qnorm"><a href="#qnorm" class="headerlink" title="qnorm"></a>qnorm</h2><p><strong>q</strong> - 指的是分位数函数(quantile function)</p>
<p>简单来说它就是pnorm的反函数, 通过百分比算z-score, 我知道分位数求z-score, 例如:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在标准正态分布中求z-score</span></span><br><span class="line">qnorm<span class="punctuation">(</span><span class="number">0.5</span><span class="punctuation">)</span> <span class="comment"># 0</span></span><br><span class="line">qnorm<span class="punctuation">(</span><span class="number">0.96</span><span class="punctuation">)</span> <span class="comment"># 1.750686</span></span><br><span class="line">qnorm<span class="punctuation">(</span><span class="number">0.99</span><span class="punctuation">)</span> <span class="comment"># 2.326348</span></span><br></pre></td></tr></table></figure>

<h2 id="rnorm"><a href="#rnorm" class="headerlink" title="rnorm"></a>rnorm</h2><p><strong>r</strong> - 指的是随机数函数(random function)（常用于概率仿真）</p>
<p>它是用来生成一组符合正态分布的随机数, 例如:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置随机数种子</span></span><br><span class="line">set.seed<span class="punctuation">(</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 生成5个符合标准正态分布的随机数</span></span><br><span class="line">rnorm<span class="punctuation">(</span><span class="number">5</span><span class="punctuation">)</span> <span class="comment"># -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078</span></span><br><span class="line"><span class="comment"># 生成10个mean=70, sd=5的正态分布随机数</span></span><br><span class="line">rnorm<span class="punctuation">(</span><span class="number">10</span><span class="punctuation">,</span> mean<span class="operator">=</span><span class="number">70</span><span class="punctuation">,</span> sd<span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span> <span class="comment"># 65.89766 72.43715 73.69162 72.87891 68.47306 77.55891 71.94922 66.89380 58.92650 75.62465</span></span><br></pre></td></tr></table></figure>



<p>在R语言中生成别的各种分布也都是以d, p, q, r开头, 原理和正态分布相似</p>
<h2 id="references"><a href="#references" class="headerlink" title="references"></a>references</h2><p><a target="_blank" rel="noopener" href="http://www.360doc.com/content/18/0913/18/19913717_786412696.shtml">http://www.360doc.com/content/18/0913/18/19913717_786412696.shtml</a></p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/r/r-basic-operators.html">https://www.runoob.com/r/r-basic-operators.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/09/R%E8%AF%AD%E8%A8%80-dnorm-pnorm-qnorm-rnorm%E7%9A%84%E5%8C%BA%E5%88%AB/" data-id="ckzi5l9p80001i5scg4c61r0w" data-title="R语言 dnorm, pnorm, qnorm, rnorm的区别" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Hexo d部署报错之spawn failed的解决方案" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/09/Hexo%20d%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99%E4%B9%8Bspawn%20failed%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" class="article-date">
  <time class="dt-published" datetime="2021-05-09T01:25:28.000Z" itemprop="datePublished">2021-05-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/09/Hexo%20d%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99%E4%B9%8Bspawn%20failed%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">Hexo d部署报错之spawn failed的解决方案</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>关于Hexo部署的时候报错导致无法推送到github估计是很多小伙伴第一次接触Hexo框架编写博客的常见问题, 下面介绍两种解决方案.</p>
<img src="https://i.loli.net/2021/05/09/fsRDw1AS2VpO35o.png">

<h2 id="解决方案-一"><a href="#解决方案-一" class="headerlink" title="解决方案(一)"></a>解决方案(一)</h2><ol>
<li>在博客文件夹(通常是<strong>\blog</strong>)中删除时 <strong>.deploy_git</strong> 文件</li>
<li>命令行(terminal)[不推荐使用<strong>cmd</strong>, 使用 <strong>git bash</strong> 等] 中输入 <code>git config --global core.autocrlf false</code>把git加入系统环境变量</li>
<li>重新执行<code>hexo c</code> <code>hexo g</code>  <code>hexo d</code></li>
</ol>
<p>上Google百度一查大部分都是这种方法, xdm可以自己试试看万一成了呢. 但我下面推荐另一种可能的解决方案</p>
<h2 id="解决方案-二"><a href="#解决方案-二" class="headerlink" title="解决方案(二)"></a>解决方案(二)</h2><ol>
<li><p>首先用文本编辑器(我使用的是Notepad++)打开博客文件夹(通常是<strong>\blog</strong>)中的 <strong>_config.yml</strong> 配置文件 </p>
</li>
<li><p>修改配置文件中的<strong>repo</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/one-command-deployment</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo:	https://github.com/YourName/YourName.github.io.git(不要使用这个)</span><br><span class="line">  		git@github.com:YourName/YourName.github.io.git(用这个)</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新执行<code>hexo c</code> <code>hexo g</code>  <code>hexo d</code></p>
</li>
</ol>
<p>这样就大功告成啦, 很简单吧, 继续写你的博客吧!</p>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p><a target="_blank" rel="noopener" href="https://blog.zhheo.com/p/128998ac.html">https://blog.zhheo.com/p/128998ac.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/njc_sec/article/details/89021083">https://blog.csdn.net/njc_sec/article/details/89021083</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/09/Hexo%20d%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99%E4%B9%8Bspawn%20failed%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" data-id="ckzi5l9p50000i5sc9mc5fmrz" data-title="Hexo d部署报错之spawn failed的解决方案" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ggplot2/" rel="tag">ggplot2</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Pytorch/" style="font-size: 20px;">Pytorch</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/ggplot2/" style="font-size: 10px;">ggplot2</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/12/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">卷积神经网络模型</a>
          </li>
        
          <li>
            <a href="/2021/08/29/Kaggle%20-%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB(CIFAR-10)/">Kaggle--图像分类(CIFAR-10) 基于Pytorch的实现</a>
          </li>
        
          <li>
            <a href="/2021/05/11/ggplot2-cheat-sheet-%E8%BD%AC%E8%BD%BD/">ggplot2 cheat sheet (转载)</a>
          </li>
        
          <li>
            <a href="/2021/05/09/R%E8%AF%AD%E8%A8%80-dnorm-pnorm-qnorm-rnorm%E7%9A%84%E5%8C%BA%E5%88%AB/">R语言 dnorm, pnorm, qnorm, rnorm的区别</a>
          </li>
        
          <li>
            <a href="/2021/05/09/Hexo%20d%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99%E4%B9%8Bspawn%20failed%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">Hexo d部署报错之spawn failed的解决方案</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>